{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contents:\n",
    "## 1. Introduction to Data Preprocessing\n",
    "## 2. Standardizing Data\n",
    "## 3. Feature Engineering\n",
    "## 4. Selecting features for modeling\n",
    "## 5. Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiking = pd.read_json('https://assets.datacamp.com/production/repositories/1816/datasets/4f26c48451bdbf73db8a58e226cd3d6b45cf7bb5/hiking.json')\n",
    "wine = pd.read_csv('https://assets.datacamp.com/production/repositories/1816/datasets/9bd5350dfdb481e0f94eeef6acf2663452a8ef8b/wine_types.csv')\n",
    "ufo = pd.read_csv('https://assets.datacamp.com/production/repositories/1816/datasets/a5ebfe5d2ed194f2668867603b563963af4769e9/ufo_sightings_large.csv')\n",
    "volunteer = pd.read_csv('https://assets.datacamp.com/production/repositories/1816/datasets/668b96955d8b252aa8439c7602d516634e3f015e/volunteer_opportunities.csv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Introduction to Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n",
      "(617, 35)\n"
     ]
    }
   ],
   "source": [
    "# Check how many values are missing in the category_desc column\n",
    "print(volunteer['category_desc'].isnull().sum())\n",
    "\n",
    "# Subset the volunteer dataset\n",
    "volunteer_subset = volunteer[volunteer['category_desc'].notnull()]\n",
    "\n",
    "# Print out the shape of the subset\n",
    "print(volunteer_subset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    737\n",
      "1     22\n",
      "2     62\n",
      "3     14\n",
      "4     31\n",
      "Name: hits, dtype: int64\n",
      "opportunity_id          int64\n",
      "content_id              int64\n",
      "vol_requests            int64\n",
      "event_time              int64\n",
      "title                  object\n",
      "hits                    int64\n",
      "summary                object\n",
      "is_priority            object\n",
      "category_id           float64\n",
      "category_desc          object\n",
      "amsl                  float64\n",
      "amsl_unit             float64\n",
      "org_title              object\n",
      "org_content_id          int64\n",
      "addresses_count         int64\n",
      "locality               object\n",
      "region                 object\n",
      "postalcode            float64\n",
      "primary_loc           float64\n",
      "display_url            object\n",
      "recurrence_type        object\n",
      "hours                   int64\n",
      "created_date           object\n",
      "last_modified_date     object\n",
      "start_date_date        object\n",
      "end_date_date          object\n",
      "status                 object\n",
      "Latitude              float64\n",
      "Longitude             float64\n",
      "Community Board       float64\n",
      "Community Council     float64\n",
      "Census Tract          float64\n",
      "BIN                   float64\n",
      "BBL                   float64\n",
      "NTA                   float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Print the head of the hits column\n",
    "print(volunteer[\"hits\"].head())\n",
    "\n",
    "# Convert the hits column to type int\n",
    "volunteer[\"hits\"] = volunteer[\"hits\"].astype(int)\n",
    "\n",
    "# Look at the dtypes of the dataset\n",
    "print(volunteer.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1.2 Class distribution - Stratified sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strengthening Communities    230\n",
      "Helping Neighbors in Need     89\n",
      "Education                     69\n",
      "Health                        39\n",
      "Environment                   24\n",
      "Emergency Preparedness        11\n",
      "Name: category_desc, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Stratified sampling\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create a data with all columns except category_desc\n",
    "volunteer_X = volunteer.drop('category_desc', axis=1)\n",
    "\n",
    "# Create a category_desc labels dataset\n",
    "volunteer_y = volunteer[['category_desc']]\n",
    "\n",
    "# Use stratified sampling to split up the dataset according to the volunteer_y dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(volunteer_X, volunteer_y, stratify=volunteer_y)\n",
    "\n",
    "# Print out the category_desc counts on the training y labels\n",
    "print(y_train['category_desc'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Standardizing Data\n",
    "\n",
    "What is standardization?\n",
    "- Scikit-learn models assume normally distributed data\n",
    "- Log normalization and feature scaling in this course\n",
    "- Applied to continuous numerical data\n",
    "\n",
    "When to standardize: models\n",
    "- Model in linear space\n",
    "- Dataset features have high variance\n",
    "- Dataset features are continuous and on different scales. Not for categorical data.\n",
    "- Linearity assumptions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Modeling without normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7333333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "wine_X = wine[['Proline','Total phenols','Hue','Nonflavanoid phenols']]\n",
    "wine_y = wine['Type']\n",
    "\n",
    "# Split the dataset and labels into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(wine_X, wine_y)\n",
    "\n",
    "# instantiate Knn classifier \n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Fit the k-nearest neighbors model to the training data\n",
    "knn.fit(X_train,y_train)\n",
    "\n",
    "# Score the model on the test data\n",
    "print(knn.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Log normalization\n",
    "\n",
    "<img src = './Images/PP-log.png' width = 400 align = \"left\"  >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type                                0.600679\n",
      "Alcohol                             0.659062\n",
      "Malic acid                          1.248015\n",
      "Ash                                 0.075265\n",
      "Alcalinity of ash                  11.152686\n",
      "Magnesium                         203.989335\n",
      "Total phenols                       0.391690\n",
      "Flavanoids                          0.997719\n",
      "Nonflavanoid phenols                0.015489\n",
      "Proanthocyanins                     0.327595\n",
      "Color intensity                     5.374449\n",
      "Hue                                 0.052245\n",
      "OD280/OD315 of diluted wines        0.504086\n",
      "Proline                         99166.717355\n",
      "dtype: float64\n",
      "\n",
      " variance of the Proline column 99166.71735542428\n",
      "\n",
      " variance of the normalized Proline column 0.17231366191842018\n"
     ]
    }
   ],
   "source": [
    "# creating copy of wine\n",
    "wine_copy = wine.copy()\n",
    "\n",
    "# checking variance in each col\n",
    "print(wine_copy.var())\n",
    "\n",
    "# Print out the variance of the Proline column\n",
    "print('\\n variance of the Proline column',wine_copy['Proline'].var())\n",
    "\n",
    "# Apply the log normalization function to the Proline column\n",
    "wine_copy['Proline_log'] = np.log(wine_copy['Proline'])\n",
    "\n",
    "# Check the variance of the normalized Proline column\n",
    "print('\\n variance of the normalized Proline column',wine_copy['Proline_log'].var())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Scaling\n",
    "\n",
    "What is feature scaling?\n",
    "- Features on different scales\n",
    "- Model with linear characteristics\n",
    "- Center features around 0 and transform to unit variance\n",
    "- Transforms to approximately normal distribution\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import StandardScaler from scikit-learn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create the scaler\n",
    "ss = StandardScaler()\n",
    "\n",
    "# Take a subset of the DataFrame you want to scale \n",
    "wine_subset = wine[['Ash','Alcalinity of ash','Magnesium']]\n",
    "\n",
    "# Apply the scaler to the DataFrame subset\n",
    "wine_subset_scaled = ss.fit_transform(wine_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 KNN on non-scaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7333333333333333\n"
     ]
    }
   ],
   "source": [
    "wine_x2 = wine.drop('Type',axis=1)\n",
    "wine_y2 = wine['Type']\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Split the dataset and labels into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(wine_x2,wine_y2)\n",
    "\n",
    "# instantiate Knn classifier \n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Fit the k-nearest neighbors model to the training data\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Score the model on the test data\n",
    "print(knn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 KNN on scaled data\n",
    "\n",
    "- The accuracy score on the unscaled wine dataset was decent, but we can likely do better if we scale the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9555555555555556\n",
      "\n",
      " The increase in accuracy is worth the extra step of scaling the dataset.\n"
     ]
    }
   ],
   "source": [
    "# Create the scaling method.\n",
    "ss = StandardScaler()\n",
    "\n",
    "# Apply the scaling method to the dataset used for modeling.\n",
    "X_scaled = ss.fit_transform(wine_x2)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, wine_y2)\n",
    "\n",
    "# Fit the k-nearest neighbors model to the training data.\n",
    "knn.fit(X_train,y_train)\n",
    "\n",
    "# Score the model on the test data.\n",
    "print(knn.score(X_test,y_test))\n",
    "\n",
    "print('\\n The increase in accuracy is worth the extra step of scaling the dataset.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Encoding categorical variables - binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Accessible_enc Accessible\n",
      "0               1          Y\n",
      "1               0          N\n",
      "2               0          N\n",
      "3               0          N\n",
      "4               0          N\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Set up the LabelEncoder object\n",
    "enc = LabelEncoder()\n",
    "\n",
    "# creating a copy of hiking data\n",
    "hiking_copy = hiking.copy()\n",
    "\n",
    "# Apply the encoding to the \"Accessible\" column\n",
    "hiking_copy['Accessible_enc'] = enc.fit_transform(hiking_copy['Accessible'])\n",
    "\n",
    "# Compare the two columns\n",
    "print(hiking_copy[['Accessible_enc', 'Accessible']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Education  Emergency Preparedness  Environment  Health  \\\n",
      "0          0                       0            0       0   \n",
      "1          0                       0            0       0   \n",
      "2          0                       0            0       0   \n",
      "3          0                       0            0       0   \n",
      "4          0                       0            1       0   \n",
      "\n",
      "   Helping Neighbors in Need  Strengthening Communities  \n",
      "0                          0                          0  \n",
      "1                          0                          1  \n",
      "2                          0                          1  \n",
      "3                          0                          1  \n",
      "4                          0                          0  \n"
     ]
    }
   ],
   "source": [
    "# Transform the category_desc column\n",
    "category_enc = pd.get_dummies(volunteer.copy()[\"category_desc\"])\n",
    "\n",
    "# Take a look at the encoded columns\n",
    "print(category_enc.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3.2 Engineering numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  start_date_converted  start_date_month\n",
      "0           2011-07-30                 7\n",
      "1           2011-02-01                 2\n",
      "2           2011-01-29                 1\n",
      "3           2011-02-14                 2\n",
      "4           2011-02-05                 2\n"
     ]
    }
   ],
   "source": [
    "volunteer_start = volunteer.copy()\n",
    "\n",
    "# First, convert string column to date column\n",
    "volunteer_start[\"start_date_converted\"] = pd.to_datetime(volunteer_start[\"start_date_date\"])\n",
    "\n",
    "# Extract just the month from the converted column\n",
    "volunteer_start[\"start_date_month\"] = volunteer_start[\"start_date_converted\"].apply(lambda row: row.month)\n",
    "\n",
    "# Take a look at the converted and new month columns\n",
    "print(volunteer_start[['start_date_converted', 'start_date_month']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3.3 Engineering features from strings - extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Length  Length_num\n",
      "0   0.8 miles        0.80\n",
      "1    1.0 mile        1.00\n",
      "2  0.75 miles        0.75\n",
      "3   0.5 miles        0.50\n",
      "4   0.5 miles        0.50\n"
     ]
    }
   ],
   "source": [
    "hiking_mile = hiking.copy()\n",
    "hiking_mile = hiking_mile[hiking_mile['Length'].notna()]\n",
    "\n",
    "import re \n",
    "\n",
    "# Write a pattern to extract numbers and decimals\n",
    "def return_mileage(length):\n",
    "    pattern = re.compile(r\"\\d+\\.\\d+\")\n",
    "    \n",
    "    # Search the text for matches\n",
    "    mile = re.match(pattern, length)\n",
    "    \n",
    "    # If a value is returned, use group(0) to return the found value\n",
    "    if mile is not None:\n",
    "        return float(mile.group(0))\n",
    "        \n",
    "# Apply the function to the Length column and take a look at both columns\n",
    "hiking_mile[\"Length_num\"] = hiking_mile[\"Length\"].apply(lambda row: return_mileage(row))\n",
    "print(hiking_mile[[\"Length\", \"Length_num\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Engineering features from strings - tf/idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.535483870967742\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "\n",
    "# Take the title text\n",
    "title_text = volunteer[volunteer['category_desc'].notna()]['title']\n",
    "\n",
    "# Create the vectorizer method\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "\n",
    "# Transform the text into tf-idf vectors\n",
    "text_tfidf = tfidf_vec.fit_transform(title_text)\n",
    "\n",
    "# Split the dataset according to the class distribution of category_desc\n",
    "y = volunteer[\"category_desc\"].dropna()\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_tfidf.toarray(), y, stratify=y)\n",
    "\n",
    "# instantiate nb algo\n",
    "nb = GaussianNB()\n",
    "# Fit the model to the training data\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "# Print out the model's accuracy\n",
    "print(nb.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Selecting features for modeling\n",
    "\n",
    "- Redundant features\n",
    "- Remove noisy features\n",
    "- Remove correlated features\n",
    "- Remove duplicated features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Selecting relevant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "volunteer_rf = volunteer[['vol_requests','title','hits','category_desc','locality','region','postalcode','created_date']]\n",
    "volunteer_rf['vol_requests_lognorm'] = np.log(volunteer_rf['vol_requests'])\n",
    "\n",
    "# First, convert string column to date column\n",
    "volunteer_rf[\"created_date\"] = pd.to_datetime(volunteer_rf[\"created_date\"])\n",
    "\n",
    "# creating month col\n",
    "volunteer_rf[\"created_month\"] = volunteer_rf[\"created_date\"].apply(lambda row: row.month)\n",
    "\n",
    "# get dummies\n",
    "category_enc = pd.get_dummies(volunteer_rf['category_desc'])\n",
    "\n",
    "# concat for final df\n",
    "volunteer_rf = pd.concat([volunteer_rf,category_enc],axis=1)\n",
    "\n",
    "# treating na\n",
    "volunteer_rf = volunteer_rf[volunteer_rf['category_desc'].notna()]\n",
    "volunteer_rf = volunteer_rf[volunteer_rf['postalcode'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title  hits  postalcode  \\\n",
      "1                                       Web designer    22     10010.0   \n",
      "2      Urban Adventures - Ice Skating at Lasker Rink    62     10026.0   \n",
      "3  Fight global hunger and support women farmers ...    14      2114.0   \n",
      "4                                      Stop 'N' Swap    31     10455.0   \n",
      "5                               Queens Stop 'N' Swap   135     11372.0   \n",
      "\n",
      "   vol_requests_lognorm  created_month  Education  Emergency Preparedness  \\\n",
      "1              0.693147              1          0                       0   \n",
      "2              2.995732              1          0                       0   \n",
      "3              6.214608              1          0                       0   \n",
      "4              2.708050              1          0                       0   \n",
      "5              2.708050              1          0                       0   \n",
      "\n",
      "   Environment  Health  Helping Neighbors in Need  Strengthening Communities  \n",
      "1            0       0                          0                          1  \n",
      "2            0       0                          0                          1  \n",
      "3            0       0                          0                          1  \n",
      "4            1       0                          0                          0  \n",
      "5            1       0                          0                          0  \n"
     ]
    }
   ],
   "source": [
    "# Create a list of redundant column names to drop\n",
    "to_drop = [\"locality\", \"region\", \"category_desc\", \"created_date\", \"vol_requests\"]\n",
    "\n",
    "# Drop those columns from the dataset\n",
    "volunteer_subset = volunteer_rf.drop(to_drop, axis=1)\n",
    "\n",
    "# Print out the head of the new dataset\n",
    "print(volunteer_subset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Checking for correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_cor = wine[['Flavanoids','Total phenols','Malic acid','OD280/OD315 of diluted wines','Hue']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              Flavanoids  Total phenols  Malic acid  \\\n",
      "Flavanoids                      1.000000       0.864564   -0.411007   \n",
      "Total phenols                   0.864564       1.000000   -0.335167   \n",
      "Malic acid                     -0.411007      -0.335167    1.000000   \n",
      "OD280/OD315 of diluted wines    0.787194       0.699949   -0.368710   \n",
      "Hue                             0.543479       0.433681   -0.561296   \n",
      "\n",
      "                              OD280/OD315 of diluted wines       Hue  \n",
      "Flavanoids                                        0.787194  0.543479  \n",
      "Total phenols                                     0.699949  0.433681  \n",
      "Malic acid                                       -0.368710 -0.561296  \n",
      "OD280/OD315 of diluted wines                      1.000000  0.565468  \n",
      "Hue                                               0.565468  1.000000  \n",
      "\n",
      " Dropping correlated features is often an iterative process, so you may need to try different combinations in your model.\n"
     ]
    }
   ],
   "source": [
    "# Print out the column correlations of the wine dataset\n",
    "print(wine_cor.corr())\n",
    "\n",
    "# Take a minute to find the column where the correlation value is greater than 0.75 at least twice\n",
    "to_drop = \"Flavanoids\"\n",
    "\n",
    "# Drop that column from the DataFrame\n",
    "wine_cor = wine_cor.drop(to_drop, axis=1)\n",
    "\n",
    "print('\\n Dropping correlated features is often an iterative process, so you may need to try different combinations in your model.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Selecting features using text vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refer to the course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2 PCA - principal component analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.99996392e-01 3.07041250e-06 4.16606366e-07 1.21244332e-07]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Set up PCA and the X vector for diminsionality reduction\n",
    "pca = PCA()\n",
    "wine_pca = wine.drop(\"Type\", axis=1)\n",
    "wine_pca_y = wine['Type']\n",
    "\n",
    "# Apply PCA to the wine dataset X vector\n",
    "transformed_X = pca.fit_transform(wine_X)\n",
    "\n",
    "# Look at the percentage of variance explained by the different components\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6444444444444445"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Split the transformed X and the y labels into training and test sets\n",
    "X_wine_train, X_wine_test, y_wine_train, y_wine_test = train_test_split(transformed_X,wine_pca_y)\n",
    "\n",
    "# Fit knn to the training data\n",
    "knn.fit(X_wine_train,y_wine_train)\n",
    "\n",
    "# Score knn on the test data and print it out\n",
    "knn.score(X_wine_test,y_wine_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Bringing it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date               object\n",
      "city               object\n",
      "state              object\n",
      "country            object\n",
      "type               object\n",
      "seconds           float64\n",
      "length_of_time     object\n",
      "desc               object\n",
      "recorded           object\n",
      "lat                object\n",
      "long              float64\n",
      "dtype: object\n",
      "\n",
      " seconds           float64\n",
      "date       datetime64[ns]\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "ufo_at = ufo.copy()\n",
    "\n",
    "# Check the column types\n",
    "print(ufo_at.dtypes)\n",
    "\n",
    "# Change the type of seconds to float\n",
    "ufo_at[\"seconds\"] = ufo_at[\"seconds\"].astype(float)\n",
    "\n",
    "# Change the date column to type datetime\n",
    "ufo_at[\"date\"] = pd.to_datetime(ufo_at[\"date\"])\n",
    "\n",
    "# Check the column types\n",
    "print('\\n',ufo_at[[\"seconds\", \"date\"]].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length_of_time    143\n",
      "state             419\n",
      "type              159\n",
      "dtype: int64\n",
      "\n",
      " (4283, 11)\n"
     ]
    }
   ],
   "source": [
    "# Check how many values are missing in the length_of_time, state, and type columns\n",
    "print(ufo_at[['length_of_time', 'state', 'type']].isnull().sum())\n",
    "\n",
    "# Keep only rows where length_of_time, state, and type are not null\n",
    "ufo_no_missing = ufo_at[ufo_at['length_of_time'].notnull() & \n",
    "          ufo_at['state'].notnull() & \n",
    "          ufo_at['type'].notnull()]\n",
    "\n",
    "# Print out the shape of the new dataset\n",
    "print('\\n',ufo_no_missing.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    length_of_time  minutes\n",
      "0          2 weeks      2.0\n",
      "1           30sec.     30.0\n",
      "3  about 5 minutes      NaN\n",
      "4                2      2.0\n",
      "5       10 minutes     10.0\n"
     ]
    }
   ],
   "source": [
    "def return_minutes(time_string):\n",
    "\n",
    "    # Use \\d+ to grab digits\n",
    "    pattern = re.compile(r\"\\d+\")\n",
    "    \n",
    "    # Use match on the pattern and column\n",
    "    num = re.match(pattern, time_string)\n",
    "    if num is not None:\n",
    "        return int(num.group(0))\n",
    "        \n",
    "# Apply the extraction to the length_of_time column\n",
    "ufo_no_missing[\"minutes\"] = ufo_no_missing[\"length_of_time\"].apply(lambda x: return_minutes(x))\n",
    "\n",
    "# Take a look at the head of both of the columns\n",
    "print(ufo_no_missing[['length_of_time','minutes']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seconds    1.545212e+10\n",
      "minutes    9.470577e+02\n",
      "dtype: float64\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "# Check the variance of the seconds and minutes columns\n",
    "print(ufo_no_missing[['seconds','minutes']].var())\n",
    "\n",
    "# Log normalize the seconds column\n",
    "ufo_no_missing[\"seconds_log\"] = np.log(ufo_no_missing[\"seconds\"])\n",
    "\n",
    "# Print out the variance of just the seconds_log column\n",
    "print(ufo_no_missing['seconds_log'].var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    }
   ],
   "source": [
    "# Use Pandas to encode us values as 1 and others as 0\n",
    "ufo_no_missing[\"country_enc\"] = ufo_no_missing[\"country\"].apply(lambda x: 1 if x == 'us' else 0)\n",
    "\n",
    "# Print the number of unique type values\n",
    "print(len(ufo_no_missing['type'].unique()))\n",
    "\n",
    "# Create a one-hot encoded set of the type values\n",
    "type_set = pd.get_dummies(ufo_no_missing['type'])\n",
    "\n",
    "# Concatenate this set back to the ufo DataFrame\n",
    "ufo_no_missing = pd.concat([ufo_no_missing, type_set], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   2011-11-03 19:21:00\n",
      "1   2004-10-03 19:05:00\n",
      "3   2002-11-21 05:45:00\n",
      "4   2010-08-19 12:55:00\n",
      "5   2012-06-16 23:00:00\n",
      "Name: date, dtype: datetime64[ns]\n",
      "\n",
      "                  date  month  year\n",
      "0 2011-11-03 19:21:00     11  2011\n",
      "1 2004-10-03 19:05:00     10  2004\n",
      "3 2002-11-21 05:45:00     11  2002\n",
      "4 2010-08-19 12:55:00      8  2010\n",
      "5 2012-06-16 23:00:00      6  2012\n"
     ]
    }
   ],
   "source": [
    "# Look at the first 5 rows of the date column\n",
    "print(ufo_no_missing['date'].head())\n",
    "\n",
    "# Extract the month from the date column\n",
    "ufo_no_missing[\"month\"] = ufo_no_missing[\"date\"].apply(lambda row: row.month)\n",
    "\n",
    "# Extract the year from the date column\n",
    "ufo_no_missing[\"year\"] = ufo_no_missing[\"date\"].apply(lambda row: row.year)\n",
    "\n",
    "# Take a look at the head of all three columns\n",
    "print('\\n',ufo_no_missing[['date','month','year']].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Red blinking objects similar to airplanes or s...\n",
      "1                 Many fighter jets flying towards UFO\n",
      "3    It was a large&#44 triangular shaped flying ob...\n",
      "4       A white spinning disc in the shape of an oval.\n",
      "5    Dancing lights that would fly around and then ...\n",
      "Name: desc, dtype: object\n",
      "\n",
      " (4283, 5754)\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the head of the desc field\n",
    "print(ufo_no_missing[\"desc\"].head())\n",
    "\n",
    "# Create the tfidf vectorizer object\n",
    "vec = TfidfVectorizer()\n",
    "\n",
    "# Use vec's fit_transform method on the desc field\n",
    "desc_tfidf = vec.fit_transform(ufo_no_missing[\"desc\"])\n",
    "\n",
    "# Look at the number of columns this creates\n",
    "print('\\n',desc_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_to_filter(vocab, original_vocab, vector, top_n):\n",
    "    filter_list = []\n",
    "    for i in range(0, vector.shape[0]):\n",
    "    \n",
    "        # Here we'll call the function from the previous exercise, and extend the list we're creating\n",
    "        filtered = return_weights(vocab, original_vocab, vector, i, top_n)\n",
    "        filter_list.extend(filtered)\n",
    "    # Return the list in a set, so we don't get duplicate word indices\n",
    "    return set(filter_list)\n",
    "\n",
    "\n",
    "vocab = {v:k for k,v in \n",
    "   vec.vocabulary_.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              seconds  seconds_log   minutes\n",
      "seconds      1.000000     0.174331 -0.009932\n",
      "seconds_log  0.174331     1.000000  0.111460\n",
      "minutes     -0.009932     0.111460  1.000000\n"
     ]
    }
   ],
   "source": [
    "# Check the correlation between the seconds, seconds_log, and minutes columns\n",
    "print(ufo_no_missing[['seconds','seconds_log','minutes']].corr())\n",
    "\n",
    "# Make a list of features to drop\n",
    "to_drop = ['city','country','country_enc','date','desc','lat','length_of_time','long','minutes','recorded','seconds','state','seconds_log']\n",
    "\n",
    "# Drop those features\n",
    "ufo_dropped = ufo_no_missing.drop(to_drop,axis=1)\n",
    "\n",
    "# Let's also filter some words out of the text vector we created\n",
    "filtered_words = words_to_filter(vocab, vec.vocabulary_, desc_tfidf, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['changing', 'chevron', 'cigar', 'circle', 'cone', 'cross', 'cylinder',\n",
      "       'diamond', 'disk', 'egg', 'fireball', 'flash', 'formation', 'light',\n",
      "       'other', 'oval', 'rectangle', 'sphere', 'teardrop', 'triangle',\n",
      "       'unknown', 'month', 'year'],\n",
      "      dtype='object')\n",
      "\n",
      " 0.811391223155929\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Creating feature and target variable\n",
    "ufo_dropped_X = ufo_dropped.drop('type',axis=1)\n",
    "ufo_dropped_y = ufo_dropped['type']\n",
    "\n",
    "# Take a look at the features in the X set of data\n",
    "print(ufo_dropped_X.columns)\n",
    "\n",
    "# Split the X and y sets using train_test_split, setting stratify=y\n",
    "train_X, test_X, train_y, test_y = train_test_split(ufo_dropped_X,ufo_dropped_y,stratify=ufo_dropped_y)\n",
    "\n",
    "# instantiate knn classifier\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Fit knn to the training sets\n",
    "knn.fit(train_X,train_y)\n",
    "\n",
    "# Print the score of knn on the test sets\n",
    "print('\\n',knn.score(test_X,test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
