{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents:\n",
    "## 1. Classification and Regression Trees\n",
    "## 2. The Bias-Variance Tradeoff\n",
    "## 3. Bagging and Random Forests\n",
    "## 4. Boosting\n",
    "## 5. Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing datasets\n",
    "auto = pd.read_csv(\"https://assets.datacamp.com/production/repositories/1796/datasets/3781d588cf7b04b1e376c7e9dda489b3e6c7465b/auto.csv\")\n",
    "bikes = pd.read_csv(\"https://assets.datacamp.com/production/repositories/1796/datasets/594538f54a854b322d6e4c8031f3f31bc522d3e5/bikes.csv\")\n",
    "wbc = pd.read_csv(\"https://assets.datacamp.com/production/repositories/1796/datasets/0eb6987cb9633e4d6aa6cfd11e00993d2387caa4/wbc.csv\")\n",
    "ilp = pd.read_csv('./Data/Indian Liver Patient Dataset (ILPD).csv')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Classification and Regression Trees\n",
    "\n",
    "A classification tree divides the feature space into rectangular regions. In contrast, a linear model such as logistic regression produces only a single linear decision boundary dividing the feature space into two decision regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Building basic Decision Tree for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_w = wbc[['radius_mean','concave points_mean']]\n",
    "y_w = wbc['diagnosis'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_w, y_w, test_size = 0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B' 'M' 'M' 'B' 'B']\n"
     ]
    }
   ],
   "source": [
    "## Training the model\n",
    "# Import DecisionTreeClassifier from sklearn.tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Instantiate a DecisionTreeClassifier 'dt' with a maximum depth of 6\n",
    "dt = DecisionTreeClassifier(max_depth=6, random_state=1)\n",
    "\n",
    "# Fit dt to the training set\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred = dt.predict(X_test)\n",
    "print(y_pred[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.89\n"
     ]
    }
   ],
   "source": [
    "## Evaluation\n",
    "# Import accuracy_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred = dt.predict(X_test)\n",
    "\n",
    "# Compute test set accuracy  \n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Test set accuracy: {:.2f}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, Logistic regression has linear decision boundary. Decision trees have rectangular non linear boundaries.\n",
    "<img src = './Images/DT-Co.png' width = 400 align = \"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Classification tree learning\n",
    "<img src = './Images/DT-Dt1.png' width = 400 align = \"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = './Images/DT-DT2.png ' width = 400 align = \"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Changing the Information gain criterion\n",
    "How do we change the criterion of information gain in the code? \n",
    "There are 2 types of criterion used for measuring information gain - entropy, gini_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy achieved by using entropy:  0.9035087719298246\n",
      "Accuracy achieved by using the gini index:  0.9035087719298246\n",
      "\n",
      "Notice how the two models achieve exactly the same accuracy. \n",
      "Most of the time, the gini index and entropy lead to the same results. \n",
      "The gini index is slightly faster to compute and is the default criterion used in the DecisionTreeClassifier model of scikit-learn.\n"
     ]
    }
   ],
   "source": [
    "# Import DecisionTreeClassifier from sklearn.tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Instantiate dt_entropy, set 'entropy' as the information criterion\n",
    "dt_entropy = DecisionTreeClassifier(max_depth=8, criterion='entropy', random_state=1)\n",
    "\n",
    "# Fit dt_entropy to the training set\n",
    "dt_entropy.fit(X_train, y_train)\n",
    "\n",
    "# Predicting \n",
    "y_pred = dt_entropy.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy_entropy\n",
    "accuracy_entropy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Instantiate dt_entropy, set 'entropy' as the information criterion\n",
    "dt_gini = DecisionTreeClassifier(max_depth=8, criterion='gini', random_state=1)\n",
    "\n",
    "# Fit dt_entropy to the training set\n",
    "dt_gini.fit(X_train, y_train)\n",
    "\n",
    "# Predicting \n",
    "y_pred = dt_gini.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy_entropy\n",
    "accuracy_gini = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print accuracy_entropy\n",
    "print('Accuracy achieved by using entropy: ', accuracy_entropy)\n",
    "\n",
    "# Print accuracy_gini\n",
    "print('Accuracy achieved by using the gini index: ', accuracy_gini)\n",
    "\n",
    "print('\\nNotice how the two models achieve exactly the same accuracy. \\nMost of the time, the gini index and entropy lead to the same results. \\nThe gini index is slightly faster to compute and is the default criterion used in the DecisionTreeClassifier model of scikit-learn.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Building basic Decision Tree for Regression\n",
    "\n",
    "Decision trees can also be used for regression problems. Below you can see that there is clearly a non linear relation between the X and y variable. A normal linearr regression would do poorly in this case. However, we can see that a decision tree performs better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = './Images/DT-reg1.png' width = 500 height = 600 align = \"left\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new Bool columns for categorical variables\n",
    "df_region = pd.get_dummies(auto)\n",
    "\n",
    "# Creating feature and target variable\n",
    "X_a = df_region.drop('mpg',axis=1)\n",
    "y_a = df_region['mpg'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set RMSE of dt: 4.82\n"
     ]
    }
   ],
   "source": [
    "# Import DecisionTreeRegressor from sklearn.tree\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_a, y_a, test_size = 0.2, random_state=42)\n",
    "\n",
    "# Instantiate dt\n",
    "dt = DecisionTreeRegressor(max_depth=8,\n",
    "             min_samples_leaf=0.13,\n",
    "            random_state=3)\n",
    "\n",
    "# Fit dt to the training set\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Compute y_pred\n",
    "y_pred = dt.predict(X_test)\n",
    "\n",
    "# Compute mse_dt\n",
    "mse_dt = MSE(y_test, y_pred)\n",
    "\n",
    "# Compute rmse_dt\n",
    "rmse_dt = mse_dt ** (1/2)\n",
    "\n",
    "# Print rmse_dt\n",
    "print(\"Test set RMSE of dt: {:.2f}\".format(rmse_dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression test set RMSE: 4.30\n",
      "Regression Tree test set RMSE: 4.82\n"
     ]
    }
   ],
   "source": [
    "## Comparing with Linear regression\n",
    "# Import LinearRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Create the regressor: reg\n",
    "reg = LinearRegression()\n",
    "\n",
    "# fitting the model\n",
    "reg.fit(X_train,y_train)\n",
    "\n",
    "# Predict test set labels \n",
    "y_pred_lr = reg.predict(X_test)\n",
    "\n",
    "# Compute mse_lr\n",
    "mse_lr = MSE(y_test, y_pred_lr)\n",
    "\n",
    "# Compute rmse_lr\n",
    "rmse_lr = mse_lr ** (1/2)\n",
    "\n",
    "# Print rmse_lr\n",
    "print('Linear Regression test set RMSE: {:.2f}'.format(rmse_lr))\n",
    "\n",
    "# Print rmse_dt\n",
    "print('Regression Tree test set RMSE: {:.2f}'.format(rmse_dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. The Bias-Variance Tradeoff\n",
    "\n",
    "The end goal of a supervised learning problem is determining function f(x) and modeling function f^ which best predicts the original function f. We also want to discard noise as much as possible and achieve a low predictive error on unseen data.\n",
    "\n",
    "<img src = './Images/DT-SL1.png' width = 300  align = \"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are usually 2 types of problems we encounter while achieving this:\n",
    "1. Overfitting\n",
    "2. Underrfitting\n",
    "\n",
    "<img src = './Images/DT-SL2.png' width = 500 align = \"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = './Images/DT-SL3.png' width = 500 align = \"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = './Images/DT-SL4.png' width = 500 align = \"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This below example is of a model that is undefitting. The model is not flexible enough to predict f(x)\n",
    "\n",
    "<img src = './Images/DT-SL5.png' width = 500 align = \"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example of a model that follows the taining points so closely it misses f(x). This leads to overfitting.\n",
    "\n",
    "\n",
    "<img src = './Images/DT-SL6.png' width = 500 align = \"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we can see that as model complexity increases, the variance increases and bias decreases and vice versa. Our goal is to create model that balances the 2.\n",
    "\n",
    "<img src = './Images/DT-SL7.png' width = 500 align = \"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = './Images/DT-SL8.png' width = 800 align = \"left\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV RMSE: 5.14\n",
      "Train RMSE: 5.15\n",
      "Notice how the training error is roughly equal to the 10-folds CV error\n"
     ]
    }
   ],
   "source": [
    "# Import train_test_split from sklearn.model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Set SEED for reproducibility\n",
    "SEED = 1\n",
    "\n",
    "# Split the data into 70% train and 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_a, y_a, test_size=0.3, random_state=SEED)\n",
    "\n",
    "# Instantiate a DecisionTreeRegressor dt\n",
    "dt = DecisionTreeRegressor(max_depth=4, min_samples_leaf=0.26, random_state=SEED)\n",
    "\n",
    "# Compute the array containing the 10-folds CV MSEs\n",
    "MSE_CV_scores = - cross_val_score(dt, X_train, y_train, cv=10, \n",
    "                       scoring='neg_mean_squared_error',\n",
    "                       n_jobs=-1)\n",
    "\n",
    "# Compute the 10-folds CV RMSE\n",
    "RMSE_CV = (MSE_CV_scores.mean())**(1/2)\n",
    "\n",
    "# Print RMSE_CV\n",
    "print('CV RMSE: {:.2f}'.format(RMSE_CV))\n",
    "#  A very good practice is to keep the test set untouched until you are confident about your model's performance. \n",
    "# CV is a great technique to get an estimate of a model's performance without affecting the test set\n",
    "\n",
    "# Fit dt to the training set\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the training set\n",
    "y_pred_train = dt.predict(X_train)\n",
    "\n",
    "# Evaluate the training set RMSE of dt\n",
    "RMSE_train = (mean_squared_error(y_train, y_pred_train))**(1/2)\n",
    "\n",
    "# Print RMSE_train\n",
    "print('Train RMSE: {:.2f}'.format(RMSE_train))\n",
    "print('Notice how the training error is roughly equal to the 10-folds CV error')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Ensemble learning\n",
    "\n",
    "<img src = './Images/DT-CT1.png' width = 500 align = \"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = './Images/DT-EL1.png' width = 500 align = \"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = './Images/DT-EL2.png' width = 500 align = \"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = './Images/DT-EL3.png' width = 500 align = \"left\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>tot_bilirubin</th>\n",
       "      <th>direct_bilirubin</th>\n",
       "      <th>tot_proteins</th>\n",
       "      <th>albumin</th>\n",
       "      <th>ag_ratio</th>\n",
       "      <th>sgpt</th>\n",
       "      <th>sgot</th>\n",
       "      <th>alkphos</th>\n",
       "      <th>is_patient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.1</td>\n",
       "      <td>187</td>\n",
       "      <td>16</td>\n",
       "      <td>18</td>\n",
       "      <td>6.8</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "      <td>10.9</td>\n",
       "      <td>5.5</td>\n",
       "      <td>699</td>\n",
       "      <td>64</td>\n",
       "      <td>100</td>\n",
       "      <td>7.5</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0.74</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "      <td>7.3</td>\n",
       "      <td>4.1</td>\n",
       "      <td>490</td>\n",
       "      <td>60</td>\n",
       "      <td>68</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0.89</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>182</td>\n",
       "      <td>14</td>\n",
       "      <td>20</td>\n",
       "      <td>6.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>72</td>\n",
       "      <td>1</td>\n",
       "      <td>3.9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>195</td>\n",
       "      <td>27</td>\n",
       "      <td>59</td>\n",
       "      <td>7.3</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  gender  tot_bilirubin  direct_bilirubin  tot_proteins  albumin  \\\n",
       "0   65       0            0.7               0.1           187       16   \n",
       "1   62       1           10.9               5.5           699       64   \n",
       "2   62       1            7.3               4.1           490       60   \n",
       "3   58       1            1.0               0.4           182       14   \n",
       "4   72       1            3.9               2.0           195       27   \n",
       "\n",
       "   ag_ratio  sgpt  sgot  alkphos  is_patient  \n",
       "0        18   6.8   3.3     0.90           1  \n",
       "1       100   7.5   3.2     0.74           1  \n",
       "2        68   7.0   3.3     0.89           1  \n",
       "3        20   6.8   3.4     1.00           1  \n",
       "4        59   7.3   2.4     0.40           1  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleaning up the dataset for alignment with DataCamp\n",
    "ilp.loc[ilp.gender == 'Male', 'gender'] = 1\n",
    "ilp.loc[ilp.gender == 'Female', 'gender'] = 0\n",
    "ilp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "ilp_1 = ilp.copy()\n",
    "ilp_1 = ilp_1.dropna()\n",
    "X_i = ilp_1.drop('is_patient',axis=1)\n",
    "y_i = ilp_1['is_patient']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression : 0.638\n",
      "K Nearest Neighbours : 0.655\n",
      "Classification Tree : 0.664\n",
      "Voting Classifier: 0.638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "### 2.2.1 Building Basic Ensemble learning model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set seed for reproducibility\n",
    "SEED=1\n",
    "\n",
    "# Creating tarining and testing sets\n",
    "# Split into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_i, y_i, test_size = 0.2, random_state=42)\n",
    "\n",
    "# Instantiate lr\n",
    "lr = LogisticRegression(random_state=SEED)\n",
    "\n",
    "# Instantiate knn\n",
    "knn = KNN(n_neighbors=27)\n",
    "\n",
    "# Instantiate dt\n",
    "dt = DecisionTreeClassifier(min_samples_leaf=0.13, random_state=SEED)\n",
    "\n",
    "# Define the list classifiers\n",
    "classifiers = [('Logistic Regression', lr), ('K Nearest Neighbours', knn), ('Classification Tree', dt)]\n",
    "\n",
    "# Iterate over the pre-defined list of classifiers\n",
    "for clf_name, clf in classifiers:    \n",
    "  \n",
    "    # Fit clf to the training set\n",
    "    clf.fit(X_train, y_train)    \n",
    "  \n",
    "    # Predict y_pred\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "  \n",
    "    # Evaluate clf's accuracy on the test set\n",
    "    print('{:s} : {:.3f}'.format(clf_name, accuracy))\n",
    "    \n",
    "# Instantiate a VotingClassifier vc\n",
    "vc = VotingClassifier(estimators=classifiers)     \n",
    "\n",
    "# Fit vc to the training set\n",
    "vc.fit(X_train, y_train)   \n",
    "\n",
    "# Evaluate the test set predictions\n",
    "y_pred = vc.predict(X_test)\n",
    "\n",
    "# Calculate accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Voting Classifier: {:.3f}'.format(accuracy))\n",
    "\n",
    "# there are some discrepancies between this dataset and the one on datacamp.\n",
    "# voting Classifier there performs better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging\n",
    "\n",
    "In the earlier example, voting classifier trained different algorithms on same training set.\n",
    "Bagging in training one algorithm on subsets of training sets. The algorithm itself can be any - Decision Tree, Logistic regression, Neural network.\n",
    "Bagging is Bootstrap aggregation. This reduces variance.\n",
    "\n",
    "Below is an example of bagging. Here, original set contains of 3 balls A,B,C. Boostrap sample is a sample drawn from this with replacement. In sample 1, B was drawn thrice. In sample 2, A was drawn twice and B once and so on.\n",
    "\n",
    "<img src = './Images/DT-BG1.png' width = 500 align = \"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While training different bootstrap samples are drawn and `n` models of same algorithm are trained on each of the samples.\n",
    "<img src = './Images/DT-BG2.png' width = 500 align = \"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When new sample is fed to the models, each model outputs it's prediction and the classifier then makes its own prediction depending on the nature of the problem. For Classification it is majority voting. For regression it is averaging the outputs.\n",
    "\n",
    "<img src = './Images/DT-BG3.png' width = 500 align = \"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Building Bagging classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy of bc: 0.61\n"
     ]
    }
   ],
   "source": [
    "# Import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Import BaggingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# Instantiate dt\n",
    "dt = DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "# Instantiate bc\n",
    "bc = BaggingClassifier(base_estimator=dt, n_estimators=50, random_state=1)\n",
    "\n",
    "# Import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Import BaggingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# Instantiate dt\n",
    "dt = DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "# Instantiate bc\n",
    "# n_estimators is the number of classification trees that will be crerated under the hood.\n",
    "bc = BaggingClassifier(base_estimator=dt, n_estimators=50, random_state=1)\n",
    "\n",
    "# Fit bc to the training set\n",
    "bc.fit(X_train, y_train)\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred = bc.predict(X_test)\n",
    "\n",
    "# Evaluate acc_test\n",
    "acc_test = accuracy_score(y_test, y_pred)\n",
    "print('Test set accuracy of bc: {:.2f}'.format(acc_test)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Out of bag Evaluation\n",
    "\n",
    "In the previous example, on an average 63% of training instances are sampled while 37% constitute OOB instances i.e. Out of Bag. These OOBs can be used to evaluate model's performance without the need for CV.\n",
    "\n",
    "In the below diagram, each model is trained on the training instances and evluated on the OOB samples. We get N scores and the OOB scoe is average of these scores.\n",
    "\n",
    "only a subset of DTs is used for determining the OOB score.This leads to reducing the overall aggregation effect in bagging. Thus in general, validation on a full ensemble of DTs is better than a subset of DT for estimating the score. However, occasionally the dataset is not big enough and hence set aside a part of it for validation is unaffordable. Consequently, in cases where we do not have a large dataset and want to consume it all as the training dataset, the OOB score provides a good trade-off. Nonetheless, it should be noted that validation score and OOB score are unalike, computed in a different manner and should not be thus compared.\n",
    "\n",
    "<img src = './Images/DT-OB1.png' width = 500 align = \"left\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.638, OOB accuracy: 0.717\n"
     ]
    }
   ],
   "source": [
    "# Import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Import BaggingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Instantiate dt\n",
    "dt = DecisionTreeClassifier(min_samples_leaf=8, random_state=1)\n",
    "\n",
    "# Instantiate bc\n",
    "bc = BaggingClassifier(base_estimator=dt, \n",
    "            n_estimators=50,\n",
    "            oob_score=True,\n",
    "            random_state=1)\n",
    "\n",
    "# Fit bc to the training set \n",
    "bc.fit(X_train, y_train)\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred = bc.predict(X_test)\n",
    "\n",
    "# Evaluate test set accuracy\n",
    "acc_test = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Evaluate OOB accuracy\n",
    "acc_oob = bc.oob_score_\n",
    "\n",
    "# Print acc_test and acc_oob\n",
    "print('Test set accuracy: {:.3f}, OOB accuracy: {:.3f}'.format(acc_test, acc_oob))\n",
    "\n",
    "# the whole point of OOB accuracy is to understand how the model will perform on unseen data without performing CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Random Forest\n",
    "\n",
    "- In Bagging, we train model on all feautres wth replacement. \n",
    "- In Random forest, we train the model with d features (d less than total features) without replacement. Also unlike bagging, the base estimator in Random forest is always decision tree. \n",
    "- Models are trained on d features and the nodes are split by features that maximize information gain. \n",
    "- d usually defaults to sqaure root of total features.\n",
    "- Random forecast can be used for Classification and Regression.\n",
    "\n",
    "<img src = './Images/DT-RF1.png' width = 500 align = \"left\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = './Images/DT-RF2.png' width = 500 align = \"left\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_b = bikes.drop('cnt',axis=1)\n",
    "y_b = bikes['cnt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set RMSE of rf: 57.66\n"
     ]
    }
   ],
   "source": [
    "# Import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# Creating tarining and testing sets\n",
    "# Split into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_b, y_b, test_size = 0.2, random_state=42)\n",
    "\n",
    "# Instantiate rf\n",
    "rf = RandomForestRegressor(n_estimators=25,\n",
    "            random_state=2)\n",
    "            \n",
    "# Fit rf to the training set    \n",
    "rf.fit(X_train, y_train) \n",
    "\n",
    "\n",
    "\n",
    "# Predict the test set labels\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Evaluate the test set RMSE\n",
    "rmse_test = MSE(y_test,y_pred)**(1/2)\n",
    "\n",
    "# Print rmse_test\n",
    "print('Test set RMSE of rf: {:.2f}'.format(rmse_test))\n",
    "# The MSE of RF is way lower than that of a single tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Visualizing features importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apparently, hr and workingday are the most important features according to rf. The importances of these two features add up to more than 90%!\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Create a pd.Series of features importances\n",
    "importances = pd.Series(data=rf.feature_importances_,\n",
    "                        index= X_train.columns)\n",
    "\n",
    "# Sort importances\n",
    "importances_sorted = importances.sort_values()\n",
    "\n",
    "# Draw a horizontal barplot of importances_sorted\n",
    "importances_sorted.plot(kind='barh', color='lightgreen')\n",
    "plt.title('Features Importances')\n",
    "plt.show()\n",
    "print('Apparently, hr and workingday are the most important features according to rf. The importances of these two features add up to more than 90%!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Boosting\n",
    "\n",
    "- Ensemble method where many models trained and several weak learners are combined to form a strong learner in way where each learner learns from the errors of its predecessor.\n",
    "- A weak learner is one which performs only slightly better than random guessing\n",
    "- This usually involves training perdictors sequentially and each predictor tried to correct errors of predecessor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Adaboost\n",
    "\n",
    "<img src = './Images/DT-AB1.png' width = 500 align = \"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process flow for training the Adaboost is:\n",
    "1. Predictor 1 is trained on dataset (X,y)\n",
    "2. Error is computed\n",
    "3. Error is used to compute alpha1, coeeficient of Predictor 1\n",
    "4. alpha1 then decides the weights of training instances of Predictor2. Incorrectly trained instances are given more weight as shown in the figure. This means the predictor would have to pay special attention to these instances.\n",
    "5. The above process is repeated sequentially till end.\n",
    "\n",
    "\n",
    "<img src = './Images/DT-AB2.png' width = 500 align = \"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have a learrning rate between 0,1.\n",
    "Ideally, smaller the value of eeta (learning rate), higher the number of estimators.\n",
    "\n",
    "<img src = './Images/DT-AB3.png' width = 500 align = \"left\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC score: 0.67\n"
     ]
    }
   ],
   "source": [
    "# Import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# Import AdaBoostClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "# Import roc_auc_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "# Creating tarining and testing sets\n",
    "# Split into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_i, y_i, test_size = 0.2, random_state=42)\n",
    "\n",
    "# Instantiate dt\n",
    "dt = DecisionTreeClassifier(max_depth=2, random_state=1)\n",
    "\n",
    "# Instantiate ada\n",
    "# n_estimators is the number of trees\n",
    "ada = AdaBoostClassifier(base_estimator=dt, n_estimators=180, random_state=1)\n",
    "\n",
    "# Fit ada to the training set\n",
    "ada.fit(X_train,y_train)\n",
    "\n",
    "# Compute the probabilities of obtaining the positive class\n",
    "y_pred_proba = ada.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Evaluate test-set roc_auc_score\n",
    "ada_roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "# Print roc_auc_score\n",
    "print('ROC AUC score: {:.2f}'.format(ada_roc_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Gradient Boosting (GB)\n",
    "\n",
    "1. GB, too is sequantial in nature but previous errors do not tweak training instances.\n",
    "2. Each predictor is trained using its predecessor's residual errors as label\n",
    "3. In the fig below, predictor is trained and error yhat gives r1, residual for predictor1\n",
    "4. This residual is used in predictor 2 giving r2 and so on.\n",
    "\n",
    "<img src = './Images/DT-GB1.png' width = 500 align = \"left\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set RMSE of gb: 51.359\n"
     ]
    }
   ],
   "source": [
    "# Import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Import mean_squared_error as MSE\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Creating tarining and testing sets\n",
    "# Split into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_b, y_b, test_size = 0.2, random_state=42)\n",
    "\n",
    "# Instantiate gb\n",
    "gb = GradientBoostingRegressor(max_depth=4, \n",
    "            n_estimators=200,\n",
    "            random_state=2)\n",
    "\n",
    "# Fit gb to the training set\n",
    "gb.fit(X_train,y_train)\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred = gb.predict(X_test)\n",
    "\n",
    "\n",
    "# Compute MSE\n",
    "mse_test = mean_squared_error(y_test,y_pred)\n",
    "\n",
    "# Compute RMSE\n",
    "rmse_test = mse_test**(1/2)\n",
    "\n",
    "# Print RMSE\n",
    "print('Test set RMSE of gb: {:.3f}'.format(rmse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Stochastic Gradient Boosting (SGB)\n",
    "\n",
    "1. GB has some cons. In GB, each CART is trained to find the best split points and features. This could lead to CARTs using the same split points and same features\n",
    "2. Stochastic Gradient Boosting is used to mitigate the above cons. In SGB, each tree is trained on a random subset of rows.\n",
    "3. Instances are sampled without replacement. Features, too are sampled without replacement. This increases the diversity and adds further variance to the tree ensemble.\n",
    "\n",
    "- In the below diagram:\n",
    "1. Instances are randomly sampled without replacement.\n",
    "2. This sampled data is considered fo training. \n",
    "3. Not all features are considered for training.\n",
    "4. The tree is then trained and the residual errors are computed.\n",
    "5. The residual error is multiplied by the learning rate and is fed to the next tree in the ensemble.\n",
    "\n",
    "<img src = './Images/DT-SGB1.png' width = 700 align = \"left\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set RMSE of sgbr: 52.738\n",
      "The stochastic gradient boosting regressor achieves a lower test set RMSE than the gradient boosting regressor\n"
     ]
    }
   ],
   "source": [
    "# Import GradientBoostingRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# Instantiate sgbr\n",
    "# n_estimators is number of decision stumps. A stump is a one-level decision tree. Notice max_depth = 1\n",
    "# subsample is used to sample 90% of data for training\n",
    "# max_features means each tree uses 75% of available features\n",
    "sgbr = GradientBoostingRegressor(max_depth=4, \n",
    "            subsample=0.9,\n",
    "            max_features=0.75,\n",
    "            n_estimators=200,                                \n",
    "            random_state=2)\n",
    "\n",
    "# Fit sgbr to the training set\n",
    "sgbr.fit(X_train,y_train)\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred = sgbr.predict(X_test)\n",
    "\n",
    "# Compute test set MSE\n",
    "mse_test = MSE(y_test,y_pred)\n",
    "\n",
    "# Compute test set RMSE\n",
    "rmse_test = mse_test ** (1/2)\n",
    "\n",
    "# Print rmse_test\n",
    "print('Test set RMSE of sgbr: {:.3f}'.format(rmse_test))\n",
    "\n",
    "print('The stochastic gradient boosting regressor achieves a lower test set RMSE than the gradient boosting regressor')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model Tuning\n",
    "\n",
    "- We need to tune hyperparameters for optimal model which will give us the optimal score, may it be accurracy (Classification) or Rsquare (Regression).\n",
    "- Cross Validation is used to generalize model performance.\n",
    "- There are multiple ways of tuning hyper params - GridSearch, Random Search, Bayesian Optimization and Genetic Algorithms. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set ROC AUC score: 0.677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Import GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "# Import roc_auc_score from sklearn.metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Creating tarining and testing sets\n",
    "# Split into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_i, y_i, test_size = 0.2, random_state=42)\n",
    "\n",
    "# You can get a list of available hyperrparams by  - dt.get_params\n",
    "\n",
    "# Define params_dt\n",
    "params_dt = {'max_depth': [2,3,4],\n",
    "'min_samples_leaf' : [0.12,0.14,0.16,0.18]}\n",
    "\n",
    "# Instantiate grid_dt\n",
    "grid_dt = GridSearchCV(estimator=dt,\n",
    "                       param_grid=params_dt,\n",
    "                       scoring='roc_auc',\n",
    "                       cv=5,\n",
    "                       n_jobs=-1)\n",
    "\n",
    "# fitting the object\n",
    "grid_dt.fit(X_train, y_train)\n",
    "\n",
    "# Extract the best estimator\n",
    "best_model = grid_dt.best_estimator_\n",
    "\n",
    "# Predict the test set probabilities of the positive class\n",
    "y_pred_proba = grid_dt.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Compute test_roc_auc\n",
    "test_roc_auc = roc_auc_score(y_test,y_pred_proba)\n",
    "\n",
    "# Print test_roc_auc\n",
    "print('Test set ROC AUC score: {:.3f}'.format(test_roc_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Tuning Hyper params for Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method BaseEstimator.get_params of RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='auto', max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=25, n_jobs=None,\n",
      "           oob_score=False, random_state=2, verbose=0, warm_start=False)>\n"
     ]
    }
   ],
   "source": [
    "# Lets get a list of hyperparams available for tuning\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Instantiate rf\n",
    "rf = RandomForestRegressor(n_estimators=25,\n",
    "            random_state=2)\n",
    "\n",
    "print(rf.get_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    4.5s\n",
      "[Parallel(n_jobs=-1)]: Done  81 out of  81 | elapsed:    7.5s finished\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE of best model: 59.908\n"
     ]
    }
   ],
   "source": [
    "# Import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# Creating tarining and testing sets\n",
    "# Split into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_b, y_b, test_size = 0.2, random_state=42)\n",
    "\n",
    "# Define the dictionary 'params_rf'\n",
    "params_rf = {\n",
    "    'n_estimators': [100,350,500],\n",
    "    'max_features': ['log2', 'auto', 'sqrt'],\n",
    "    'min_samples_leaf': [2, 10, 30]\n",
    "}\n",
    "\n",
    "# Instantiate grid_rf\n",
    "grid_rf = GridSearchCV(estimator=rf,\n",
    "                       param_grid=params_rf,\n",
    "                       scoring='neg_mean_squared_error',\n",
    "                       cv=3,\n",
    "                       verbose=1,\n",
    "                       n_jobs=-1)\n",
    "# fit the model\n",
    "grid_rf.fit(X_train,y_train)\n",
    "\n",
    "# Extract the best estimator\n",
    "best_model = grid_rf.best_estimator_\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Compute rmse_test\n",
    "rmse_test = MSE(y_test,y_pred)**(1/2)\n",
    "\n",
    "# Print rmse_test\n",
    "print('Test RMSE of best model: {:.3f}'.format(rmse_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
