{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introducing-Keras\" data-toc-modified-id=\"Introducing-Keras-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introducing Keras</a></span><ul class=\"toc-item\"><li><span><a href=\"#First-Neural-Net\" data-toc-modified-id=\"First-Neural-Net-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>First Neural Net</a></span><ul class=\"toc-item\"><li><span><a href=\"#Hello-nets!\" data-toc-modified-id=\"Hello-nets!-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>Hello nets!</a></span></li><li><span><a href=\"#Counting-parameters\" data-toc-modified-id=\"Counting-parameters-1.1.2\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>Counting parameters</a></span></li></ul></li><li><span><a href=\"#Surviving-a-meteor-strike\" data-toc-modified-id=\"Surviving-a-meteor-strike-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Surviving a meteor strike</a></span><ul class=\"toc-item\"><li><span><a href=\"#Specifying-a-model\" data-toc-modified-id=\"Specifying-a-model-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>Specifying a model</a></span></li></ul></li><li><span><a href=\"#Training\" data-toc-modified-id=\"Training-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Training</a></span><ul class=\"toc-item\"><li><span><a href=\"#Predicting-the-orbit!\" data-toc-modified-id=\"Predicting-the-orbit!-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>Predicting the orbit!</a></span></li></ul></li></ul></li><li><span><a href=\"#Going-Deeper\" data-toc-modified-id=\"Going-Deeper-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Going Deeper</a></span><ul class=\"toc-item\"><li><span><a href=\"#Binary-classification\" data-toc-modified-id=\"Binary-classification-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Binary classification</a></span><ul class=\"toc-item\"><li><span><a href=\"#Exploring-dollar-bills\" data-toc-modified-id=\"Exploring-dollar-bills-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Exploring dollar bills</a></span></li><li><span><a href=\"#A-binary-classification-model\" data-toc-modified-id=\"A-binary-classification-model-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>A binary classification model</a></span></li><li><span><a href=\"#Is-this-dollar-bill-fake-?\" data-toc-modified-id=\"Is-this-dollar-bill-fake-?-2.1.3\"><span class=\"toc-item-num\">2.1.3&nbsp;&nbsp;</span>Is this dollar bill fake ?</a></span></li></ul></li><li><span><a href=\"#Multi-class-classification\" data-toc-modified-id=\"Multi-class-classification-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Multi-class classification</a></span><ul class=\"toc-item\"><li><span><a href=\"#throwing-darts-problem\" data-toc-modified-id=\"throwing-darts-problem-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>throwing darts problem</a></span></li><li><span><a href=\"#A-multi-class-model\" data-toc-modified-id=\"A-multi-class-model-2.2.2\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span>A multi-class model</a></span></li><li><span><a href=\"#Prepare-your-dataset\" data-toc-modified-id=\"Prepare-your-dataset-2.2.3\"><span class=\"toc-item-num\">2.2.3&nbsp;&nbsp;</span>Prepare your dataset</a></span></li><li><span><a href=\"#Training-on-dart-throwers\" data-toc-modified-id=\"Training-on-dart-throwers-2.2.4\"><span class=\"toc-item-num\">2.2.4&nbsp;&nbsp;</span>Training on dart throwers</a></span></li><li><span><a href=\"#Softmax-predictions\" data-toc-modified-id=\"Softmax-predictions-2.2.5\"><span class=\"toc-item-num\">2.2.5&nbsp;&nbsp;</span>Softmax predictions</a></span></li></ul></li><li><span><a href=\"#Multi-label-classification\" data-toc-modified-id=\"Multi-label-classification-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Multi-label classification</a></span><ul class=\"toc-item\"><li><span><a href=\"#An-irrigation-machine\" data-toc-modified-id=\"An-irrigation-machine-2.3.1\"><span class=\"toc-item-num\">2.3.1&nbsp;&nbsp;</span>An irrigation machine</a></span></li><li><span><a href=\"#Training-with-multiple-labels\" data-toc-modified-id=\"Training-with-multiple-labels-2.3.2\"><span class=\"toc-item-num\">2.3.2&nbsp;&nbsp;</span>Training with multiple labels</a></span></li></ul></li><li><span><a href=\"#Keras-callbacks\" data-toc-modified-id=\"Keras-callbacks-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Keras callbacks</a></span><ul class=\"toc-item\"><li><span><a href=\"#The-history-callback\" data-toc-modified-id=\"The-history-callback-2.4.1\"><span class=\"toc-item-num\">2.4.1&nbsp;&nbsp;</span>The history callback</a></span></li><li><span><a href=\"#Early-stopping-your-model\" data-toc-modified-id=\"Early-stopping-your-model-2.4.2\"><span class=\"toc-item-num\">2.4.2&nbsp;&nbsp;</span>Early stopping your model</a></span></li><li><span><a href=\"#A-combination-of-callbacks\" data-toc-modified-id=\"A-combination-of-callbacks-2.4.3\"><span class=\"toc-item-num\">2.4.3&nbsp;&nbsp;</span>A combination of callbacks</a></span></li></ul></li></ul></li><li><span><a href=\"#Improving-Your-Model-Performance\" data-toc-modified-id=\"Improving-Your-Model-Performance-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Improving Your Model Performance</a></span><ul class=\"toc-item\"><li><span><a href=\"#Learning-curves\" data-toc-modified-id=\"Learning-curves-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Learning curves</a></span><ul class=\"toc-item\"><li><span><a href=\"#Learning-the-digits\" data-toc-modified-id=\"Learning-the-digits-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span>Learning the digits</a></span></li><li><span><a href=\"#Is-the-model-overfitting?\" data-toc-modified-id=\"Is-the-model-overfitting?-3.1.2\"><span class=\"toc-item-num\">3.1.2&nbsp;&nbsp;</span>Is the model overfitting?</a></span></li><li><span><a href=\"#Do-we-need-more-data?\" data-toc-modified-id=\"Do-we-need-more-data?-3.1.3\"><span class=\"toc-item-num\">3.1.3&nbsp;&nbsp;</span>Do we need more data?</a></span></li></ul></li><li><span><a href=\"#Activation-functions\" data-toc-modified-id=\"Activation-functions-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Activation functions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Comparing-activation-functions\" data-toc-modified-id=\"Comparing-activation-functions-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>Comparing activation functions</a></span></li><li><span><a href=\"#Comparing-activation-functions-II\" data-toc-modified-id=\"Comparing-activation-functions-II-3.2.2\"><span class=\"toc-item-num\">3.2.2&nbsp;&nbsp;</span>Comparing activation functions II</a></span></li></ul></li><li><span><a href=\"#Batch-size-and-batch-normalization\" data-toc-modified-id=\"Batch-size-and-batch-normalization-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Batch size and batch normalization</a></span><ul class=\"toc-item\"><li><span><a href=\"#Batch-normalizing-a-familiar-model\" data-toc-modified-id=\"Batch-normalizing-a-familiar-model-3.3.1\"><span class=\"toc-item-num\">3.3.1&nbsp;&nbsp;</span>Batch normalizing a familiar model</a></span></li><li><span><a href=\"#Batch-normalization-effects\" data-toc-modified-id=\"Batch-normalization-effects-3.3.2\"><span class=\"toc-item-num\">3.3.2&nbsp;&nbsp;</span>Batch normalization effects</a></span></li></ul></li><li><span><a href=\"#Hyperparameter-tuning\" data-toc-modified-id=\"Hyperparameter-tuning-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Hyperparameter tuning</a></span><ul class=\"toc-item\"><li><span><a href=\"#Preparing-a-model-for-tuning\" data-toc-modified-id=\"Preparing-a-model-for-tuning-3.4.1\"><span class=\"toc-item-num\">3.4.1&nbsp;&nbsp;</span>Preparing a model for tuning</a></span></li><li><span><a href=\"#Tuning-the-model-parameters\" data-toc-modified-id=\"Tuning-the-model-parameters-3.4.2\"><span class=\"toc-item-num\">3.4.2&nbsp;&nbsp;</span>Tuning the model parameters</a></span></li><li><span><a href=\"#Training-with-cross-validation\" data-toc-modified-id=\"Training-with-cross-validation-3.4.3\"><span class=\"toc-item-num\">3.4.3&nbsp;&nbsp;</span>Training with cross-validation</a></span></li></ul></li></ul></li><li><span><a href=\"#Advanced-Model-Architectures\" data-toc-modified-id=\"Advanced-Model-Architectures-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Advanced Model Architectures</a></span><ul class=\"toc-item\"><li><span><a href=\"#Tensors,-layers,-and-autoencoders\" data-toc-modified-id=\"Tensors,-layers,-and-autoencoders-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Tensors, layers, and autoencoders</a></span><ul class=\"toc-item\"><li><span><a href=\"#It's-a-flow-of-tensors\" data-toc-modified-id=\"It's-a-flow-of-tensors-4.1.1\"><span class=\"toc-item-num\">4.1.1&nbsp;&nbsp;</span>It's a flow of tensors</a></span></li><li><span><a href=\"#Neural-separation\" data-toc-modified-id=\"Neural-separation-4.1.2\"><span class=\"toc-item-num\">4.1.2&nbsp;&nbsp;</span>Neural separation</a></span></li><li><span><a href=\"#Building-an-autoencoder\" data-toc-modified-id=\"Building-an-autoencoder-4.1.3\"><span class=\"toc-item-num\">4.1.3&nbsp;&nbsp;</span>Building an autoencoder</a></span></li><li><span><a href=\"#De-noising-like-an-autoencoder\" data-toc-modified-id=\"De-noising-like-an-autoencoder-4.1.4\"><span class=\"toc-item-num\">4.1.4&nbsp;&nbsp;</span>De-noising like an autoencoder</a></span></li></ul></li><li><span><a href=\"#Intro-to-CNNs\" data-toc-modified-id=\"Intro-to-CNNs-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Intro to CNNs</a></span><ul class=\"toc-item\"><li><span><a href=\"#Building-a-CNN-model\" data-toc-modified-id=\"Building-a-CNN-model-4.2.1\"><span class=\"toc-item-num\">4.2.1&nbsp;&nbsp;</span>Building a CNN model</a></span></li><li><span><a href=\"#Looking-at-convolutions\" data-toc-modified-id=\"Looking-at-convolutions-4.2.2\"><span class=\"toc-item-num\">4.2.2&nbsp;&nbsp;</span>Looking at convolutions</a></span></li><li><span><a href=\"#Preparing-your-input-image\" data-toc-modified-id=\"Preparing-your-input-image-4.2.3\"><span class=\"toc-item-num\">4.2.3&nbsp;&nbsp;</span>Preparing your input image</a></span></li><li><span><a href=\"#Using-a-real-world-model\" data-toc-modified-id=\"Using-a-real-world-model-4.2.4\"><span class=\"toc-item-num\">4.2.4&nbsp;&nbsp;</span>Using a real world model</a></span></li></ul></li><li><span><a href=\"#Intro-to-LSTMs\" data-toc-modified-id=\"Intro-to-LSTMs-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Intro to LSTMs</a></span><ul class=\"toc-item\"><li><span><a href=\"#Text-prediction-with-LSTMs\" data-toc-modified-id=\"Text-prediction-with-LSTMs-4.3.1\"><span class=\"toc-item-num\">4.3.1&nbsp;&nbsp;</span>Text prediction with LSTMs</a></span></li><li><span><a href=\"#Build-your-LSTM-model\" data-toc-modified-id=\"Build-your-LSTM-model-4.3.2\"><span class=\"toc-item-num\">4.3.2&nbsp;&nbsp;</span>Build your LSTM model</a></span></li><li><span><a href=\"#Decode-your-predictions\" data-toc-modified-id=\"Decode-your-predictions-4.3.3\"><span class=\"toc-item-num\">4.3.3&nbsp;&nbsp;</span>Decode your predictions</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing Keras\n",
    "\n",
    "## First Neural Net\n",
    "\n",
    "### Hello nets!\n",
    "You're going to build a simple neural network to get a feeling of how quickly it is to accomplish this in Keras.\n",
    "\n",
    "You will build a network that takes two numbers as an input, passes them through a hidden layer of 10 neurons, and finally outputs a single non-constrained number.\n",
    "\n",
    "A non-constrained output can be obtained by avoiding setting an activation function in the output layer. This is useful for problems like regression, when we want our output to be able to take any non-constrained value.\n",
    "\n",
    "<img src = './Images/IDK-1.png' width = 300 align = \"left\">"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Import the Sequential model and Dense layer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Create a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add an input layer and a hidden layer with 10 neurons\n",
    "model.add(Dense(10, input_shape=(2,), activation=\"relu\"))\n",
    "\n",
    "# Add a 1-neuron output layer\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Summarise your model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting parameters\n",
    "You've just created a neural network. But you're going to create a new one now, taking some time to think about the weights of each layer. The Keras Dense layer and the Sequential model are already loaded for you to use.\n",
    "\n",
    "This is the network you will be creating:\n",
    "\n",
    "<img src = './Images/IDK-2.png' width = 300 align = \"left\">"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Instantiate a new Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add a Dense layer with five neurons and three inputs\n",
    "model.add(Dense(5, input_shape=(3,), activation=\"relu\"))\n",
    "\n",
    "# Add a final Dense layer with one neuron and no activation\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Summarize your model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Surviving a meteor strike\n",
    "\n",
    "### Specifying a model\n",
    "You will build a simple regression model to predict the orbit of the meteor!\n",
    "\n",
    "Your training data consist of measurements taken at time steps from -10 minutes before the impact region to +10 minutes after. Each time step can be viewed as an X coordinate in our graph, which has an associated position Y for the meteor orbit at that time step.\n",
    "\n",
    "Note that you can view this problem as approximating a quadratic function via the use of neural networks.\n",
    "\n",
    "This data is stored in two numpy arrays: one called time_steps , what we call features, and another called y_positions, with the labels. Go on and build your model! It should be able to predict the y positions for the meteor orbit at future time steps.\n",
    "\n",
    "Keras Sequential model and Dense layers are available for you to use.\n",
    "\n",
    "<img src = './Images/IDK-3.jpg' width = 300 align = \"left\">"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Instantiate a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add a Dense layer with 50 neurons and an input of 1 neuron\n",
    "model.add(Dense(50, input_shape=(1,), activation='relu'))\n",
    "\n",
    "# Add two Dense layers with 50 neurons and relu activation\n",
    "model.add(Dense(50,activation='relu'))\n",
    "model.add(Dense(50,activation='relu'))\n",
    "\n",
    "# End your model with a Dense layer and no activation\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "You're going to train your first model in this course, and for a good cause!\n",
    "\n",
    "Remember that before training your Keras models you need to compile them. This can be done with the .compile() method. The .compile() method takes arguments such as the optimizer, used for weight updating, and the loss function, which is what we want to minimize. Training your model is as easy as calling the .fit() method, passing on the features, labels and a number of epochs to train for.\n",
    "\n",
    "The regression model you built in the previous exercise is loaded for you to use, along with the time_steps and y_positions data. Train it and evaluate it on this very same data, let's see if your model can learn the meteor's trajectory."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Compile your model\n",
    "model.compile(loss = 'mse', optimizer = 'adam')\n",
    "\n",
    "print(\"Training started..., this can take a while:\")\n",
    "\n",
    "# Fit your model on your data for 30 epochs\n",
    "model.fit(time_steps,y_positions, epochs = 30)\n",
    "\n",
    "# Evaluate your model \n",
    "print(\"Final loss value:\",model.evaluate(time_steps, y_positions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting the orbit!\n",
    "You've already trained a model that approximates the orbit of the meteor approaching Earth and it's loaded for you to use.\n",
    "\n",
    "Since you trained your model for values between -10 and 10 minutes, your model hasn't yet seen any other values for different time steps. You will now visualize how your model behaves on unseen data.\n",
    "\n",
    "If you want to check the source code of plot_orbit, paste show_code(plot_orbit) into the console."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Instantiate a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add a Dense layer with 50 neurons and an input of 1 neuron\n",
    "model.add(Dense(50, input_shape=(1,), activation='relu'))\n",
    "\n",
    "# Add two Dense layers with 50 neurons and relu activation\n",
    "model.add(Dense(50,activation='relu'))\n",
    "model.add(Dense(50,activation='relu'))\n",
    "\n",
    "# End your model with a Dense layer and no activation\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile your model\n",
    "model.compile(loss = 'mse', optimizer = 'adam')\n",
    "\n",
    "print(\"Training started..., this can take a while:\")\n",
    "\n",
    "# Fit your model on your data for 30 epochs\n",
    "model.fit(time_steps,y_positions, epochs = 30)\n",
    "\n",
    "# Evaluate your model \n",
    "print(\"Final loss value:\",model.evaluate(time_steps, y_positions))\n",
    "\n",
    "# Predict the twenty minutes orbit\n",
    "twenty_min_orbit = model.predict(np.arange(-10, 11))\n",
    "\n",
    "# Plot the twenty minute orbit \n",
    "plot_orbit(twenty_min_orbit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Going Deeper\n",
    "\n",
    "\n",
    "## Binary classification\n",
    "\n",
    "### Exploring dollar bills\n",
    "You will practice building classification models in Keras with the Banknote Authentication dataset.\n",
    "\n",
    "Your goal is to distinguish between real and fake dollar bills. In order to do this, the dataset comes with 4 features: variance,skewness,kurtosis and entropy. These features are calculated by applying mathematical operations over the dollar bill images. The labels are found in the dataframe's class column.\n",
    "\n",
    "\n",
    "<img src = './Images/IDK-4.png' width = 300 align = \"left\">"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Import seaborn\n",
    "import seaborn as sns\n",
    "\n",
    "# Use pairplot and set the hue to be our class column\n",
    "sns.pairplot(banknotes, hue='class') \n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Describe the data\n",
    "print('Dataset stats: \\n', banknotes.describe())\n",
    "\n",
    "# Count the number of observations per class\n",
    "print('Observations per class: \\n', banknotes['class'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A binary classification model\n",
    "Now that you know what the Banknote Authentication dataset looks like, we'll build a simple model to distinguish between real and fake bills.\n",
    "\n",
    "You will perform binary classification by using a single neuron as an output. The input layer will have 4 neurons since we have 4 features in our dataset. The model's output will be a value constrained between 0 and 1.\n",
    "\n",
    "We will interpret this output number as the probability of our input variables coming from a fake dollar bill, with 1 meaning we are certain it's a fake bill.\n",
    "\n",
    "<img src = './Images/IDK-4.jpeg' width = 300 align = \"left\">"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Import the sequential model and dense layer\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "# Create a sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add a dense layer \n",
    "model.add(Dense(1, input_shape=(4,), activation='sigmoid'))\n",
    "\n",
    "# Compile your model\n",
    "model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "# Display a summary of your model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is this dollar bill fake ?\n",
    "You are now ready to train your model and check how well it performs when classifying new bills! The dataset has already been partitioned into features: X_train & X_test, and labels: y_train & y_test."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Train your model for 20 epochs\n",
    "model.fit(X_train, y_train, epochs = 20)\n",
    "\n",
    "# Evaluate your model accuracy on the test set\n",
    "accuracy = model.evaluate(X_test, y_test)[1]\n",
    "\n",
    "# Print accuracy\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-class classification\n",
    "\n",
    "### throwing darts problem\n",
    "\n",
    "<img src = './Images/IDK-6.png' width = 300 align = \"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = './Images/IDK-7.png' width = 300 align = \"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = './Images/IDK-8.png' width = 300 align = \"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A multi-class model\n",
    "You're going to build a model that predicts who threw which dart only based on where that dart landed! (That is the dart's x and y coordinates on the board.)\n",
    "\n",
    "This problem is a multi-class classification problem since each dart can only be thrown by one of 4 competitors. So classes/labels are mutually exclusive, and therefore we can build a neuron with as many output as competitors and use the softmax activation function to achieve a total sum of probabilities of 1 over all competitors.\n",
    "\n",
    "Keras Sequential model and Dense layer are already loaded for you to use."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Instantiate a sequential model\n",
    "model = Sequential()\n",
    "  \n",
    "# Add 3 dense layers of 128, 64 and 32 neurons each\n",
    "model.add(Dense(128, input_shape=(2,), activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "  \n",
    "# Add a dense layer with as many neurons as competitors\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "  \n",
    "# Compile your model using categorical_crossentropy loss\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare your dataset\n",
    "In the console you can check that your labels, darts.competitor are not yet in a format to be understood by your network. They contain the names of the competitors as strings. You will first turn these competitors into unique numbers,then use the to_categorical() function from keras.utils to turn these numbers into their one-hot encoded representation.\n",
    "\n",
    "This is useful for multi-class classification problems, since there are as many output neurons as classes and for every observation in our dataset we just want one of the neurons to be activated.\n",
    "\n",
    "The dart's dataset is loaded as darts. Pandas is imported as pd. Let's prepare this dataset!"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Transform into a categorical variable\n",
    "darts.competitor = pd.Categorical(darts.competitor)\n",
    "\n",
    "# Assign a number to each category (label encoding)\n",
    "darts.competitor = darts.competitor.cat.codes \n",
    "\n",
    "# Import to_categorical from keras utils module\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "coordinates = darts.drop(['competitor'], axis=1)\n",
    "# Use to_categorical on your labels\n",
    "competitors = to_categorical(darts.competitor)\n",
    "\n",
    "# Now print the one-hot encoded labels\n",
    "print('One-hot encoded competitors: \\n',competitors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on dart throwers\n",
    "Your model is now ready, just as your dataset. It's time to train!\n",
    "\n",
    "The coordinates features and competitors labels you just transformed have been partitioned into coord_train,coord_test and competitors_train,competitors_test.\n",
    "\n",
    "Your model is also loaded. Feel free to visualize your training data or model.summary() in the console.\n",
    "\n",
    "Let's find out who threw which dart just by looking at the board!"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Fit your model to the training data for 200 epochs\n",
    "model.fit(coord_train,competitors_train,epochs=200)\n",
    "\n",
    "# Evaluate your model accuracy on the test data\n",
    "accuracy = model.evaluate(coord_test, competitors_test)[1]\n",
    "\n",
    "# Print accuracy\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax predictions\n",
    "Your recently trained model is loaded for you. This model is generalizing well!, that's why you got a high accuracy on the test set.\n",
    "\n",
    "Since you used the softmax activation function, for every input of 2 coordinates provided to your model there's an output vector of 4 numbers. Each of these numbers encodes the probability of a given dart being thrown by one of the 4 possible competitors.\n",
    "\n",
    "When computing accuracy with the model's .evaluate() method, your model takes the class with the highest probability as the prediction. np.argmax() can help you do this since it returns the index with the highest value in an array.\n",
    "\n",
    "Use the collection of test throws stored in coords_small_test and np.argmax()to check this out!"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Predict on coords_small_test\n",
    "preds = model.predict(coords_small_test)\n",
    "\n",
    "# Print preds vs true values\n",
    "print(\"{:45} | {}\".format('Raw Model Predictions','True labels'))\n",
    "for i,pred in enumerate(preds):\n",
    "  print(\"{} | {}\".format(pred,competitors_small_test[i]))\n",
    "\n",
    "# Extract the position of highest probability from each pred vector\n",
    "preds_chosen = [np.argmax(pred) for pred in preds]\n",
    "\n",
    "# Print preds vs true values\n",
    "print(\"{:10} | {}\".format('Rounded Model Predictions','True labels'))\n",
    "for i,pred in enumerate(preds_chosen):\n",
    "  print(\"{:25} | {}\".format(pred,competitors_small_test[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-label classification\n",
    "\n",
    "<img src = './Images/IDK-9.png' width = 300 align = \"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An irrigation machine\n",
    "You're going to automate the watering of farm parcels by making an intelligent irrigation machine. Multi-label classification problems differ from multi-class problems in that each observation can be labeled with zero or more classes. So classes/labels are not mutually exclusive, you could water all, none or any combination of farm parcels based on the inputs.\n",
    "\n",
    "To account for this behavior what we do is have an output layer with as many neurons as classes but this time, unlike in multi-class problems, each output neuron has a sigmoid activation function. This makes each neuron in the output layer able to output a number between 0 and 1 independently.\n",
    "\n",
    "Keras Sequential() model and Dense() layers are preloaded. It's time to build an intelligent irrigation machine!\n",
    "\n",
    "<img src = './Images/IDK-10.jpg' width = 300 align = \"left\">"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Instantiate a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add a hidden layer of 64 neurons and a 20 neuron's input\n",
    "model.add(Dense(64,input_shape=(20,),activation='relu'))\n",
    "\n",
    "# Add an output layer of 3 neurons with sigmoid activation\n",
    "model.add(Dense(3,activation='sigmoid'))\n",
    "\n",
    "# Compile your model with binary crossentropy loss\n",
    "model.compile(optimizer='adam',\n",
    "           loss = 'binary_crossentropy',\n",
    "           metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with multiple labels\n",
    "An output of your multi-label model could look like this: [0.76 , 0.99 , 0.66 ]. If we round up probabilities higher than 0.5, this observation will be classified as containing all 3 possible labels [1,1,1]. For this particular problem, this would mean watering all 3 parcels in your farm is the right thing to do, according to the network, given the input sensor measurements.\n",
    "\n",
    "You will now train and predict with the model you just built. sensors_train, parcels_train, sensors_test and parcels_test are already loaded for you to use."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Train for 100 epochs using a validation split of 0.2\n",
    "model.fit(sensors_train, parcels_train, epochs = 100, validation_split = 0.2)\n",
    "\n",
    "# Predict on sensors_test and round up the predictions\n",
    "preds = model.predict(sensors_test)\n",
    "preds_rounded = np.round(preds)\n",
    "\n",
    "# Print rounded preds\n",
    "print('Rounded Predictions: \\n', preds_rounded)\n",
    "\n",
    "# Evaluate your model's accuracy on the test data\n",
    "accuracy = model.evaluate(sensors_test , parcels_test)[1]\n",
    "\n",
    "# Print accuracy\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras callbacks\n",
    "\n",
    "### The history callback\n",
    "The history callback is returned by default every time you train a model with the .fit() method. To access these metrics you can access the history dictionary parameter inside the returned h_callback object with the corresponding keys.\n",
    "\n",
    "The irrigation machine model you built in the previous lesson is loaded for you to train, along with its features and labels now loaded as X_train, y_train, X_test, y_test. This time you will store the model's historycallback and use the validation_data parameter as it trains.\n",
    "\n",
    "You will plot the results stored in history with plot_accuracy() and plot_loss(), two simple matplotlib functions. You can check their code in the console by pasting show_code(plot_loss)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Train your model and save its history\n",
    "h_callback = model.fit(X_train, y_train, epochs = 50,\n",
    "               validation_data=(X_test, y_test))\n",
    "\n",
    "# Plot train vs test loss during training\n",
    "plot_loss(h_callback.history['loss'], h_callback.history['val_loss'])\n",
    "\n",
    "# Plot train vs test accuracy during training\n",
    "plot_accuracy(h_callback.history['acc'], h_callback.history['val_acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = './Images/IDK-11.svg' width = 300 align = \"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early stopping your model\n",
    "The early stopping callback is useful since it allows for you to stop the model training if it no longer improves after a given number of epochs. To make use of this functionality you need to pass the callback inside a list to the model's callback parameter in the .fit() method."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Import the early stopping callback\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define a callback to monitor val_acc\n",
    "monitor_val_acc = EarlyStopping(monitor='val_acc', \n",
    "                       patience=5)\n",
    "\n",
    "# Train your model using the early stopping callback\n",
    "model.fit(X_train, y_train, \n",
    "           epochs=1000, validation_data=(X_test,y_test),\n",
    "           callbacks= [monitor_val_acc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A combination of callbacks\n",
    "Deep learning models can take a long time to train, especially when you move to deeper architectures and bigger datasets. Saving your model every time it improves as well as stopping it when it no longer does allows you to worry less about choosing the number of epochs to train for. You can also restore a saved model anytime and resume training where you left it.\n",
    "\n",
    "The model training and validation data are available in your workspace as X_train, X_test, y_train, and y_test.\n",
    "\n",
    "Use the EarlyStopping() and the ModelCheckpoint() callbacks so that you can go eat a jar of cookies while you leave your computer to work!"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Import the EarlyStopping and ModelCheckpoint callbacks\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Early stop on validation accuracy\n",
    "monitor_val_acc = EarlyStopping(monitor = 'val_acc', patience = 3)\n",
    "\n",
    "# Save the best model as best_banknote_model.hdf5\n",
    "modelCheckpoint = ModelCheckpoint('best_banknote_model.hdf5', save_best_only = True)\n",
    "\n",
    "# Fit your model for a stupid amount of epochs\n",
    "h_callback = model.fit(X_train, y_train,\n",
    "                    epochs = 1000000000000,\n",
    "                    callbacks = [monitor_val_acc, modelCheckpoint],\n",
    "                    validation_data = (X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving Your Model Performance\n",
    "## Learning curves\n",
    "### Learning the digits\n",
    "\n",
    "Learning the digits\n",
    "You're going to build a model on the digits dataset, a sample dataset that comes pre-loaded with scikit learn. The digits dataset consist of 8x8 pixel handwritten digits from 0 to 9:\n",
    "\n",
    "\n",
    "You want to distinguish between each of the 10 possible digits given an image, so we are dealing with multi-class classification.\n",
    "The dataset has already been partitioned into X_train, y_train, X_test, and y_test, using 30% of the data as testing data. The labels are already one-hot encoded vectors, so you don't need to use Keras to_categorical() function."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Instantiate a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Input and hidden layer with input_shape, 16 neurons, and relu \n",
    "model.add(Dense(16, input_shape = (64,), activation = 'relu'))\n",
    "\n",
    "# Output layer with 10 neurons (one per digit) and softmax\n",
    "model.add(Dense(10,activation='softmax'))\n",
    "\n",
    "# Compile your model\n",
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Test if your model is well assembled by predicting before training\n",
    "print(model.predict(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is the model overfitting?\n",
    "Let's train the model you just built and plot its learning curve to check out if it's overfitting! You can make use of the loaded function plot_loss() to plot training loss against validation loss, you can get both from the history callback.\n",
    "\n",
    "If you want to inspect the plot_loss() function code, paste this in the console: show_code(plot_loss)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "def plot_loss(loss,val_loss):\n",
    "  plt.figure()\n",
    "  plt.plot(loss)\n",
    "  plt.plot(val_loss)\n",
    "  plt.title('Model loss')\n",
    "  plt.ylabel('Loss')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.legend(['Train', 'Test'], loc='upper right')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Train your model for 60 epochs, using X_test and y_test as validation data\n",
    "h_callback = model.fit(X_train, y_train, epochs = 60, validation_data = (X_test, y_test), verbose=0)\n",
    "\n",
    "# Extract from the h_callback object loss and val_loss to plot the learning curve\n",
    "plot_loss(h_callback.history['loss'], h_callback.history['val_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do we need more data?\n",
    "It's time to check whether the digits dataset model you built benefits from more training examples!\n",
    "\n",
    "In order to keep code to a minimum, various things are already initialized and ready to use:\n",
    "\n",
    "    The model you just built.\n",
    "    X_train,y_train,X_test, and y_test.\n",
    "    The initial_weights of your model, saved after using model.get_weights().\n",
    "    A pre-defined list of training sizes: training_sizes.\n",
    "    A pre-defined early stopping callback monitoring loss: early_stop.\n",
    "    Two empty lists to store the evaluation results: train_accs and test_accs.\n",
    "\n",
    "Train your model on the different training sizes and evaluate the results on X_test. End by plotting the results with plot_results()."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for size in training_sizes:\n",
    "  \t# Get a fraction of training data (we only care about the training data)\n",
    "    X_train_frac, y_train_frac = X_train[:size], y_train[:size]\n",
    "\n",
    "    # Reset the model to the initial weights and train it on the new training data fraction\n",
    "    model.set_weights(initial_weights)\n",
    "    model.fit(X_train_frac, y_train_frac, epochs = 50, callbacks = [early_stop])\n",
    "\n",
    "    # Evaluate and store both: the training data fraction and the complete test set results\n",
    "    train_accs.append(model.evaluate(X_train_frac, y_train_frac)[1])\n",
    "    test_accs.append(model.evaluate(X_test, y_test)[1])\n",
    "    \n",
    "# Plot train vs test accuracies\n",
    "plot_results(train_accs, test_accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation functions\n",
    "\n",
    "<img src = './Images/IDK-12.png' width = 300 align = \"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing activation functions\n",
    "Comparing activation functions involves a bit of coding, but nothing you can't do!\n",
    "\n",
    "You will try out different activation functions on the multi-label model you built for your farm irrigation machine in chapter 2. The function get_model('relu') returns a copy of this model and applies the 'relu' activation function to its hidden layer.\n",
    "\n",
    "You will loop through several activation functions, generate a new model for each and train it. By storing the history callback in a dictionary you will be able to visualize which activation function performed best in the next exercise!\n",
    "\n",
    "X_train, y_train, X_test, y_test are ready for you to use when training your models."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Activation functions to try\n",
    "activations = ['relu', 'leaky_relu', 'sigmoid', 'tanh']\n",
    "\n",
    "# Loop over the activation functions\n",
    "activation_results = {}\n",
    "\n",
    "for act in activations:\n",
    "  # Get a new model with the current activation\n",
    "  model = get_model(act)\n",
    "  # Fit the model and store the history results\n",
    "  h_callback = model.fit(X_train, y_train, epochs = 20, validation_data = (X_test, y_test), verbose=0)\n",
    "  activation_results[act] = h_callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing activation functions II\n",
    "What you coded in the previous exercise has been executed to obtain theactivation_results variable, this time 100 epochs were used instead of 20. This way you will have more epochs to further compare how the training evolves per activation function.\n",
    "\n",
    "For every h_callback of each activation function in activation_results:\n",
    "\n",
    "    The h_callback.history['val_loss'] has been extracted.\n",
    "    The h_callback.history['val_acc'] has been extracted.\n",
    "\n",
    "Both are saved into two dictionaries: val_loss_per_function and val_acc_per_function.\n",
    "\n",
    "Pandas is also loaded as pd for you to use. Let's plot some quick validation loss and accuracy charts!"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Create a dataframe from val_loss_per_function\n",
    "val_loss= pd.DataFrame(val_loss_per_function)\n",
    "\n",
    "# Call plot on the dataframe\n",
    "val_loss.plot()\n",
    "plt.show()\n",
    "\n",
    "# Create a dataframe from val_acc_per_function\n",
    "val_acc = pd.DataFrame(val_acc_per_function)\n",
    "\n",
    "# Call plot on the dataframe\n",
    "val_acc.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch size and batch normalization\n",
    "\n",
    "- Mini-batches Advantages: \n",
    "    - Networks train faster (more weight updates in same amount of time)\n",
    "    - Less RAM memory required, can train on huge datasets\n",
    "    - Noise can help networks reach a lower error, escaping local minima\n",
    "    \n",
    "- Mini-batches Disadvantages::\n",
    "    - More iterations need to be runNeed to be adjusted, we need to nd a good batch size\n",
    "\n",
    "\n",
    "- Batch normalization advantages:\n",
    "    - Improves gradient owAllows higher learning rates\n",
    "    - Reduces dependence on weight initializations\n",
    "    - Acts as an unintended form of regularizationLimits internal covariate shift\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch normalizing a familiar model\n",
    "Remember the digits dataset you trained in the first exercise of this chapter?\n",
    "\n",
    "A multi-class classification problem that you solved using softmax and 10 neurons in your output layer.\n",
    "You will now build a new deeper model consisting of 3 hidden layers of 50 neurons each, using batch normalization in between layers. The kernel_initializer parameter is used to initialize weights in a similar way."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Import batch normalization from keras layers\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "# Build your deep network\n",
    "batchnorm_model = Sequential()\n",
    "batchnorm_model.add(Dense(50, input_shape=(64,), activation='relu', kernel_initializer='normal'))\n",
    "batchnorm_model.add(BatchNormalization())\n",
    "batchnorm_model.add(Dense(50, activation='relu', kernel_initializer='normal'))\n",
    "batchnorm_model.add(BatchNormalization())\n",
    "batchnorm_model.add(Dense(50, activation='relu', kernel_initializer='normal'))\n",
    "batchnorm_model.add(BatchNormalization())\n",
    "batchnorm_model.add(Dense(10, activation='softmax', kernel_initializer='normal'))\n",
    "\n",
    "# Compile your model with sgd\n",
    "batchnorm_model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch normalization effects\n",
    "Batch normalization tends to increase the learning speed of our models and make their learning curves more stable. Let's see how two identical models with and without batch normalization compare.\n",
    "\n",
    "The model you just built batchnorm_model is loaded for you to use. An exact copy of it without batch normalization: standard_model, is available as well. You can check their summary() in the console. X_train, y_train, X_test, and y_test are also loaded so that you can train both models.\n",
    "\n",
    "You will compare the accuracy learning curves for both models plotting them with compare_histories_acc().\n",
    "\n",
    "You can check the function pasting show_code(compare_histories_acc) in the console."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Train your standard model, storing its history callback\n",
    "h1_callback = standard_model.fit(X_train, y_train, validation_data=(X_test,y_test), epochs=10, verbose=0)\n",
    "\n",
    "# Train the batch normalized model you recently built, store its history callback\n",
    "h2_callback = batchnorm_model.fit(X_train, y_train, validation_data=(X_test,y_test), epochs=10, verbose=0)\n",
    "\n",
    "# Call compare_histories_acc passing in both model histories\n",
    "compare_histories_acc(h1_callback, h2_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = './Images/IDK-13.svg' width = 300 align = \"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning\n",
    "### Preparing a model for tuning\n",
    "Let's tune the hyperparameters of a binary classification model that does well classifying the breast cancer dataset.\n",
    "\n",
    "You've seen that the first step to turn a model into a sklearn estimator is to build a function that creates it. The definition of this function is important since hyperparameter tuning is carried out by varying the arguments your function receives.\n",
    "\n",
    "Build a simple create_model() function that receives both a learning rate and an activation function as arguments. The Adam optimizer has been imported as an object from keras.optimizers so that you can also change its learning rate parameter."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Creates a model given an activation and learning rate\n",
    "def create_model(learning_rate, activation):\n",
    "  \n",
    "  \t# Create an Adam optimizer with the given learning rate\n",
    "  \topt = Adam(lr = learning_rate)\n",
    "  \t\n",
    "  \t# Create your binary classification model  \n",
    "  \tmodel = Sequential()\n",
    "  \tmodel.add(Dense(128, input_shape = (30,), activation = activation))\n",
    "  \tmodel.add(Dense(256, activation = activation))\n",
    "  \tmodel.add(Dense(1, activation = 'sigmoid'))\n",
    "  \t\n",
    "  \t# Compile your model with your optimizer, loss, and metrics\n",
    "  \tmodel.compile(optimizer = opt, loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "  \treturn model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning the model parameters\n",
    "It's time to try out different parameters on your model and see how well it performs!\n",
    "\n",
    "The create_model() function you built in the previous exercise is ready for you to use.\n",
    "\n",
    "Since fitting the RandomizedSearchCV object would take too long, the results you'd get are printed in the show_results() function. You could try random_search.fit(X,y) in the console yourself to check it does work after you have built everything else, but you will probably timeout the exercise (so copy your code first if you try this or you can lose your progress!).\n",
    "\n",
    "You don't need to use the optional epochs and batch_size parameters when building your KerasClassifier object since you are passing them as params to the random search and this works already."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Import KerasClassifier from keras scikit learn wrappers\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# Create a KerasClassifier\n",
    "model = KerasClassifier(build_fn = create_model)\n",
    "\n",
    "# Define the parameters to try out\n",
    "params = {'activation': ['relu', 'tanh'], 'batch_size': [32, 128, 256], \n",
    "          'epochs': [50, 100, 200], 'learning_rate': [0.1, 0.01, 0.001]}\n",
    "\n",
    "# Create a randomize search cv object passing in the parameters to try\n",
    "random_search = RandomizedSearchCV(model, param_distributions = params, cv = KFold(3))\n",
    "\n",
    "# Running random_search.fit(X,y) would start the search,but it takes too long! \n",
    "show_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with cross-validation\n",
    "Time to train your model with the best parameters found: 0.001 for the learning rate, 50 epochs, a 128 batch_size and relu activations.\n",
    "\n",
    "The create_model() function from the previous exercise is ready for you to use. X and y are loaded as features and labels.\n",
    "\n",
    "Use the best values found for your model when creating your KerasClassifier object so that they are used when performing cross_validation.\n",
    "\n",
    "End this chapter by training an awesome tuned model on the breast cancer dataset!"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Import KerasClassifier from keras wrappers\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# Create a KerasClassifier\n",
    "model = KerasClassifier(build_fn = create_model(learning_rate = 0.001, activation = 'relu'), epochs = 50, \n",
    "             batch_size = 128, verbose = 0)\n",
    "\n",
    "# Calculate the accuracy score for each fold\n",
    "kfolds = cross_val_score(model, X, y, cv = 3)\n",
    "\n",
    "# Print the mean accuracy\n",
    "print('The mean accuracy was:', kfolds.mean())\n",
    "\n",
    "# Print the accuracy standard deviation\n",
    "print('With a standard deviation of:', kfolds.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Model Architectures\n",
    "\n",
    "## Tensors, layers, and autoencoders\n",
    "\n",
    "### It's a flow of tensors\n",
    "If you have already built a model, you can use the model.layers and the keras.backend to build functions that, provided with a valid input tensor, return the corresponding output tensor.\n",
    "\n",
    "This is a useful tool when we want to obtain the output of a network at an intermediate layer.\n",
    "\n",
    "For instance, if you get the input and output from the first layer of a network, you can build an inp_to_out function that returns the result of carrying out forward propagation through only the first layer for a given input tensor.\n",
    "\n",
    "So that's what you're going to do right now!\n",
    "\n",
    "X_test from the Banknote Authentication dataset and its model are preloaded. Type model.summary() in the console to check it.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Import keras backend\n",
    "import keras.backend as K\n",
    "\n",
    "# Input tensor from the 1st layer of the model\n",
    "inp = model.layers[0].input\n",
    "\n",
    "# Output tensor from the 1st layer of the model\n",
    "out = model.layers[0].output\n",
    "\n",
    "# Define a function from inputs to outputs\n",
    "inp_to_out = K.function([inp], [out])\n",
    "\n",
    "# Print the results of passing X_test through the 1st layer\n",
    "print(inp_to_out([X_test]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural separation\n",
    "Put on your gloves because you're going to perform brain surgery!\n",
    "\n",
    "Neurons learn by updating their weights to output values that help them better distinguish between the different output classes in your dataset. You will make use of the inp_to_out() function you just built to visualize the output of two neurons in the first layer of the Banknote Authentication model as it learns.\n",
    "\n",
    "The model you built in chapter 2 is ready for you to use, just like X_test and y_test. Paste show_code(plot) in the console if you want to check plot().\n",
    "\n",
    "You're performing heavy duty, once all is done, click through the graphs to watch the separation live!"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for i in range(0, 21):\n",
    "  \t# Train model for 1 epoch\n",
    "    h = model.fit(X_train, y_train, batch_size = 16, epochs = 1, verbose = 0)\n",
    "    if i%4==0: \n",
    "      # Get the output of the first layer\n",
    "      layer_output = inp_to_out([X_test])[0]\n",
    "      \n",
    "      # Evaluate model accuracy for this epoch\n",
    "      test_accuracy = model.evaluate(X_test, y_test)[1] \n",
    "      \n",
    "      # Plot 1st vs 2nd neuron output\n",
    "      plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building an autoencoder\n",
    "Autoencoders have several interesting applications like anomaly detection or image denoising. They aim at producing an output identical to its inputs. The input will be compressed into a lower dimensional space, encoded. The model then learns to decode it back to its original form.\n",
    "\n",
    "You will encode and decode the MNIST dataset of handwritten digits, the hidden layer will encode a 32-dimensional representation of the image, which originally consists of 784 pixels (28 x 28). The autoencoder will essentially learn to turn the 784 pixels original image into a compressed 32 pixels image and learn how to use that encoded representation to bring back the original 784 pixels image.\n",
    "\n",
    "The Sequential model and Dense layers are ready for you to use."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Start with a sequential model\n",
    "autoencoder = Sequential()\n",
    "\n",
    "# Add a dense layer with input the original image pixels and neurons the encoded representation\n",
    "autoencoder.add(Dense(32, input_shape=(784, ), activation=\"relu\"))\n",
    "\n",
    "# Add an output layer with as many neurons as the orginal image pixels\n",
    "autoencoder.add(Dense(784, activation = \"sigmoid\"))\n",
    "\n",
    "# Compile your model with adadelta\n",
    "autoencoder.compile(optimizer = 'adadelta', loss = 'binary_crossentropy')\n",
    "\n",
    "# Summarize your model structure\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### De-noising like an autoencoder\n",
    "Okay, you have just built an autoencoder model. Let's see how it handles a more challenging task.\n",
    "\n",
    "First, you will build a model that encodes images, and you will check how different digits are represented with show_encodings(). To build the encoder you will make use of your autoencoder, that has already being trained. You will just use the first half of the network, which contains the input and the bottleneck output. That way, you will obtain a 32 number output which represents the encoded version of the input image.\n",
    "\n",
    "Then, you will apply your autoencoder to noisy images from MNIST, it should be able to clean the noisy artifacts.\n",
    "\n",
    "X_test_noise is loaded in your workspace. The digits in this noisy dataset look like this:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Build your encoder by using the first layer of your autoencoder\n",
    "encoder = Sequential()\n",
    "encoder.add(autoencoder.layers[0])\n",
    "\n",
    "# Encode the noisy images and show the encodings for your favorite number [0-9]\n",
    "encodings = encoder.predict(X_test_noise)\n",
    "show_encodings(encodings, number = 1)\n",
    "\n",
    "# Predict on the noisy images with your autoencoder\n",
    "decoded_imgs = autoencoder.predict(X_test_noise)\n",
    "\n",
    "# Plot noisy vs decoded images\n",
    "compare_plot(X_test_noise, decoded_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to CNNs\n",
    "\n",
    "### Building a CNN model\n",
    "Building a CNN model in Keras isn't much more difficult than building any of the models you've already built throughout the course! You just need to make use of convolutional layers.\n",
    "\n",
    "You're going to build a shallow convolutional model that classifies the MNIST digits dataset. The same one you de-noised with your autoencoder! The images are 28 x 28 pixels and just have one channel, since they are black and white pictures.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Import the Conv2D and Flatten layers and instantiate model\n",
    "from keras.layers import Conv2D,Flatten\n",
    "model = Sequential()\n",
    "\n",
    "# Add a convolutional layer of 32 filters of size 3x3\n",
    "model.add(Conv2D(32, kernel_size = 3, input_shape = (28, 28, 1), activation = 'relu'))\n",
    "\n",
    "# Add a convolutional layer of 16 filters of size 3x3\n",
    "model.add(Conv2D(16, kernel_size = 3, activation = 'relu'))\n",
    "\n",
    "# Flatten the previous layer output\n",
    "model.add(Flatten())\n",
    "\n",
    "# Add as many outputs as classes with softmax activation\n",
    "model.add(Dense(10, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at convolutions\n",
    "Inspecting the activations of a convolutional layer is a cool thing. You have to do it at least once in your lifetime!\n",
    "\n",
    "To do so, you will build a new model with the Keras Model object, which takes in a list of inputs and a list of outputs. The output you will provide to this new model is the first convolutional layer outputs when given an MNIST digit as input image.\n",
    "\n",
    "The convolutional model you built in the previous exercise has already been trained for you. It can now correctly classify MNIST handwritten images. You can check it with model.summary() in the console.\n",
    "\n",
    "Let's look at the convolutional masks that were learned in the first convolutional layer of this model!"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Obtain a reference to the outputs of the first layer\n",
    "first_layer_output = model.layers[0].output\n",
    "\n",
    "# Build a model using the model's input and the first layer output\n",
    "first_layer_model = Model(inputs = model.layers[0].input, outputs = first_layer_output)\n",
    "\n",
    "# Use this model to predict on X_test\n",
    "activations = first_layer_model.predict(X_test)\n",
    "\n",
    "# Plot the activations of first digit of X_test for the 15th filter\n",
    "axs[0].matshow(activations[0,:,:,14], cmap = 'viridis')\n",
    "\n",
    "# Do the same but for the 18th filter now\n",
    "axs[1].matshow(activations[0,:,:,18], cmap = 'viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing your input image\n",
    "The original ResNet50 model was trained with images of size 224 x 224 pixels and a number of preprocessing operations; like the subtraction of the mean pixel value in the training set for all training images. You need to pre-process the images you want to predict on in the same way.\n",
    "\n",
    "When predicting on a single image you need it to fit the model's input shape, which in this case looks like this: (batch-size, width, height, channels),np.expand_dims with parameter axis = 0 adds the batch-size dimension, representing that a single image will be passed to predict. This batch-size dimension value is 1, since we are only predicting on one image.\n",
    "\n",
    "You will go over these preprocessing steps as you prepare this dog's (named Ivy) image into one that can be classified by ResNet50."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Import image and preprocess_input\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet50 import preprocess_input\n",
    "\n",
    "# Load the image with the right target size for your model\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "\n",
    "# Turn it into an array\n",
    "img_array = image.img_to_array(img)\n",
    "\n",
    "# Expand the dimensions of the image, this is so that it fits the expected model input format\n",
    "img_expanded = np.expand_dims(img_array, axis = 0)\n",
    "\n",
    "# Pre-process the img in the same way original images were\n",
    "img_ready = preprocess_input(img_expanded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a real world model\n",
    "Okay, so Ivy's picture is ready to be used by ResNet50. It is stored in img_ready.\n",
    "\n",
    "ResNet50 is a model trained on the Imagenet dataset that is able to distinguish between 1000 different labeled objects. ResNet50 is a deep model with 50 layers, you can check it in 3D here.\n",
    "\n",
    "ResNet50 and decode_predictions have both been imported from keras.applications.resnet50 for you.\n",
    "\n",
    "It's time to use this trained model to find out Ivy's breed!"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Instantiate a ResNet50 model with 'imagenet' weights\n",
    "model = ResNet50(weights='imagenet')\n",
    "\n",
    "# Predict with ResNet50 on your already processed img\n",
    "preds = model.predict(img_ready)\n",
    "\n",
    "# Decode the first 3 predictions\n",
    "print('Predicted:', decode_predictions(preds, top=3)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to LSTMs\n",
    "\n",
    "### Text prediction with LSTMs\n",
    "During the following exercises you will build a toy LSTM model that is able to predict the next word using a small text dataset. This dataset consist of cleaned quotes from the The Lord of the Ring movies. You can find them in the text variable.\n",
    "\n",
    "You will turn this text into sequences of length 4 and make use of the Keras Tokenizer to prepare the features and labels for your model!\n",
    "\n",
    "The Keras Tokenizer is already imported for you to use. It assigns a unique number to each unique word, and stores the mappings in a dictionary. This is important since the model deals with numbers but we later will want to decode the output numbers back into words."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Split text into an array of words \n",
    "words = text.split()\n",
    "\n",
    "# Make sentences of 4 words each, moving one word at a time\n",
    "sentences = []\n",
    "for i in range(4, len(words)):\n",
    "  sentences.append(' '.join(words[i-4:i]))\n",
    "\n",
    "# Instantiate a Tokenizer, then fit it on the sentences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "# Turn sentences into a sequence of numbers\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "print(\"Sentences: \\n {} \\n Sequences: \\n {}\".format(sentences[:5],sequences[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build your LSTM model\n",
    "You've already prepared your sequences of text. It's time to build your LSTM model!\n",
    "\n",
    "Remember your sequences had 4 words each, your model will be trained on the first three words of each sequence, predicting the 4th one. You are going to use an Embedding layer that will essentially learn to turn words into vectors. These vectors will then be passed to a simple LSTM layer. Our output is a Dense layer with as many neurons as words in the vocabulary and softmax activation. This is because we want to obtain the highest probable next word out of all possible words.\n",
    "\n",
    "The size of the vocabulary of words (the unique number of words) is stored in vocab_size."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Import the Embedding, LSTM and Dense layer\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Add an Embedding layer with the right parameters\n",
    "model.add(Embedding(input_dim = vocab_size, input_length = 3, output_dim = 8, ))\n",
    "\n",
    "# Add a 32 unit LSTM layer\n",
    "model.add(LSTM(32))\n",
    "\n",
    "# Add a hidden Dense layer of 32 units and an output layer of vocab_size with softmax\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decode your predictions\n",
    "Your LSTM model has already been trained (details in the previous exercise success message) so that you don't have to wait. It's time to define a function that decodes its predictions. The trained model will be passed as a default parameter to this function.\n",
    "\n",
    "Since you are predicting on a model that uses the softmax function, numpy's argmax() can be used to obtain the index/position representing the most probable next word out of the output vector of probabilities.\n",
    "\n",
    "The tokenizer you previously created and fitted, is loaded for you. You will be making use of its internal index_word dictionary to turn the model's next word prediction (which is an integer) into the actual written word it represents.\n",
    "\n",
    "You're very close to experimenting with your model!"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def predict_text(test_text, model = model):\n",
    "  if len(test_text.split()) != 3:\n",
    "    print('Text input should be 3 words!')\n",
    "    return False\n",
    "  \n",
    "  # Turn the test_text into a sequence of numbers\n",
    "  test_seq = tokenizer.texts_to_sequences([test_text])\n",
    "  test_seq = np.array(test_seq)\n",
    "  \n",
    "  # Use the model passed as a parameter to predict the next word\n",
    "  pred = model.predict(test_seq).argmax(axis = 1)[0]\n",
    "  \n",
    "  # Return the word that maps to the prediction\n",
    "  return tokenizer.index_word[pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
